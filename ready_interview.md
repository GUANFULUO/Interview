# 1. 方差与偏差

**偏差**与**方差**分别是用于衡量一个模型**泛化误差**的两个方面；

- 模型的**偏差**，指的是模型预测的**期望值**与**真实值**之间的差；
- 模型的**方差**，指的是模型预测的**期望值**与**预测值**之间的差平方和；

**偏差**用于描述模型的**拟合能力**
**方差**用于描述模型的**稳定性**

## 1.1 导致偏差和方差的原因

- 偏差通常是由于我们对学习算法做了错误的假设，或者模型的复杂度不够；
  - 比如真实模型是一个二次函数，而我们假设模型为一次函数，这就会导致偏差的增大（欠拟合）；
  - **由偏差引起的误差**通常在**训练误差**上就能体现，或者说训练误差主要是由偏差造成的
- 方差通常是由于模型的复杂度相对于训练集过高导致的；
  - 比如真实模型是一个简单的二次函数，而我们假设模型是一个高次函数，这就会导致方差的增大（过拟合）；
  - **由方差引起的误差**通常体现在测试误差相对训练误差的**增量**上。

## 1.2 深度学习中的偏差与方差

- 神经网络的拟合能力非常强，因此它的**训练误差**（偏差）通常较小；
- 但是过强的拟合能力会导致较大的方差，使模型的测试误差（**泛化误差**）增大；
- 因此深度学习的核心工作之一就是研究如何降低模型的泛化误差，这类方法统称为**正则化方法**。

# 2. 生成模型与判别模型

- 监督学习的任务是学习一个模型，对给定的输入预测相应的输出

- 这个模型的一般形式为一个**决策函数**或一个**条件概率分布**（后验概率）：

  <div align="center"><a href="https://www.codecogs.com/eqnedit.php?latex=Y&space;=&space;f(X)&space;or&space;P(Y|X)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Y&space;=&space;f(X)&space;or&space;P(Y|X)" title="Y = f(X) or P(Y|X)" /></a></div>

  - **决策函数**：输入 X 返回 Y；其中 Y 与一个**阈值**比较，然后根据比较结果判定 X 的类别
  - **条件概率分布**：输入 X 返回 **X 属于每个类别的概率**；将其中概率最大的作为 X 所属的类别

- 监督学习模型可分为**生成模型**与**判别模型**

  - 判别模型直接学习决策函数或者条件概率分布

    - 直观来说，**判别模型**学习的是类别之间的最优分隔面，反映的是不同类数据之间的差异

  - 生成模型学习的是联合概率分布`P(X,Y)`，然后根据条件概率公式计算`P(Y|X)`

    <div align="center"><a href="https://www.codecogs.com/eqnedit.php?latex=P(Y|X)&space;=&space;\frac{P(X,Y)}{P(X)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(Y|X)&space;=&space;\frac{P(X,Y)}{P(X)}" title="P(Y|X) = \frac{P(X,Y)}{P(X)}" /></a></div>

**两者之间的联系**

- 由生成模型可以得到判别模型，但由判别模型得不到生成模型。

- 当存在“隐变量”时，只能使用生成模型

  > 隐变量：当我们找不到引起某一现象的原因时，就把这个在起作用，但无法确定的因素，叫“隐变量”

**优缺点**

- 判别模型
  - 优点
    - 直接面对预测，往往学习的准确率更高
    - 由于直接学习 `P(Y|X)` 或 `f(X)`，可以对数据进行各种程度的抽象，定义特征并使用特征，以简化学习过程
  - 缺点
    - 不能反映训练数据本身的特性
    - ...
- 生成模型
  - 优点
    - 可以还原出联合概率分布 `P(X,Y)`，判别方法不能
    - 学习收敛速度更快——即当样本容量增加时，学到的模型可以更快地收敛到真实模型
    - 当存在“隐变量”时，只能使用生成模型
  - 缺点
    - 学习和计算过程比较复杂

**常见模型**

- 判别模型
  - K 近邻、感知机（神经网络）、决策树、逻辑斯蒂回归、**最大熵模型**、SVM、提升方法、**条件随机场**
- 生成模型
  - 朴素贝叶斯、隐马尔可夫模型、混合高斯模型、贝叶斯网络、马尔可夫随机场

# 3. 先验概率与后验概率

> [先验概率，后验概率，似然概率，条件概率，贝叶斯，最大似然](https://blog.csdn.net/suranxu007/article/details/50326873) - CSDN博客

**条件概率**（似然概率）

- 一个事件发生后另一个事件发生的概率。
- 一般的形式为 `P(X|Y)`，表示 y 发生的条件下 x 发生的概率。
- 有时为了区分一般意义上的**条件概率**，也称**似然概率**

**先验概率**

- 事件发生前的预判概率
- 可以是基于历史数据的统计，可以由背景常识得出，也可以是人的主观观点给出。
- 一般都是**单独事件**发生的概率，如 `P(A)`、`P(B)`。

**后验概率**

- 基于先验概率求得的**反向条件概率**，形式上与条件概率相同（若 `P(X|Y)` 为正向，则 `P(Y|X)` 为反向）

**贝叶斯公式**

<div align="center"><a href="https://www.codecogs.com/eqnedit.php?latex=P(Y|X)&space;=&space;\frac{P(X|Y)&space;*&space;P(Y)}{P(X)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(Y|X)&space;=&space;\frac{P(X|Y)&space;*&space;P(Y)}{P(X)}" title="P(Y|X) = \frac{P(X|Y) * P(Y)}{P(X)}" /></a></div>

**最大似然理论：**

认为`P(x|y)`最大的类别`y`，就是当前文档所属类别。即`Max P(x|y) = Max p(x1|y)*p(x2|y)*...p(xn|y), for all y`

**贝叶斯理论：**

认为需要增加先验概率`p(y)`，因为有可能某个`y`是很稀有的类别几千年才看见一次，即使`P(x|y)`很高，也很可能不是它。

所以`y = Max P(x|y) * P(y)`,其中`p(y)`一般是数据集里统计出来的。


# 4. 超参数选择

