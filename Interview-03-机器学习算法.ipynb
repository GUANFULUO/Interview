{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. 逻辑斯蒂回归（Logistic Regression）\n",
    "\n",
    "\n",
    "## Reference\n",
    "\n",
    "- [Logistic Regression（逻辑回归）原理及公式推导](https://blog.csdn.net/programmer_wei/article/details/52072939) - CSDN\n",
    "\n",
    "- [逻辑回归推导](https://www.cnblogs.com/daguankele/p/6549891.html) - 罐装可乐 - 博客园\n",
    "\n",
    "- [极大似然估计详解](https://blog.csdn.net/zengxiantao1994/article/details/72787849) - CSDN\n",
    "\n",
    "- [一文搞懂极大似然估计](https://zhuanlan.zhihu.com/p/26614750) - 知乎\n",
    "\n",
    "- [梯度下降原理及Python实现](https://blog.csdn.net/programmer_wei/article/details/51941358) - CSDN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 支持向量机（SVM）\n",
    "\n",
    "SVM 和 Logistic 回归的比较：\n",
    "\n",
    "（1）经典的SVM，直接输出类别，不给出后验概率；\n",
    "\n",
    "（2）Logistic回归，会给出属于哪一个类别的后验概率；\n",
    "\n",
    "（3）比较重点是二者目标函数的异同。\n",
    "\n",
    "TODO\n",
    "\n",
    "- 凸二次优化\n",
    "- 拉格朗日乘子法\n",
    "\n",
    "## Reference\n",
    "\n",
    "- [SVM中支持向量的通俗解释](https://blog.csdn.net/AerisIceBear/article/details/79588583) - CSDN\n",
    "\n",
    "- [支持向量机SVM推导及求解过程](https://blog.csdn.net/american199062/article/details/51322852#commentBox) - CSDN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 决策树\n",
    "\n",
    "决策树的训练通常由三部分组成：\n",
    "\n",
    "- 特征选择\n",
    "- 树的生成\n",
    "- 剪枝\n",
    "\n",
    "## 信息增益与信息增益比\n",
    "\n",
    "TODO\n",
    "\n",
    "## 分类树 - ID3 决策树与 C4.5 决策树 \n",
    "\n",
    "ID3 决策树和 C4.5 决策树的区别在于：**前者使用信息增益来进行特征选择，而后者使用信息增益比。**\n",
    "\n",
    "TODO\n",
    "\n",
    "## 决策树如何避免过拟合\n",
    "\n",
    "TODO\n",
    "\n",
    "## 回归树 - CART 决策树\n",
    "\n",
    "> 《统计学习方法》 5.5 CART 算法\n",
    "\n",
    "- CART 算法是在给定输入随机变量 _`X`_ 条件下输出随机变量 _`Y`_ 的**条件概率分布**的学习方法。 \n",
    "- CART 算法假设决策树是**二叉树**，内部节点特征的取值为“**是**”和“**否**”。\n",
    "\n",
    "  这样的决策树等价于递归地二分每个特征，**将输入空间/特征空间划分为有限个单元**，然后在这些单元上确定在输入给定的条件下输出的**条件概率分布**。\n",
    "- CART 决策树**既可以用于分类，也可以用于回归**；\n",
    "\n",
    "对回归树 CART 算法用**平方误差最小化**准则来选择特征，对分类树用**基尼指数最小化**准则选择特征\n",
    "\n",
    "## Reference\n",
    "\n",
    "- []()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 集成学习\n",
    "\n",
    "- 基本思想：由多个学习器组合成一个性能更好的学习器\n",
    "- **集成学习为什么有效？**——不同的模型通常会在测试集上产生不同的误差。平均上，集成模型能至少与其任一成员表现一致；并且**如果成员的误差是独立的**，集成模型将显著地比其成员表现更好。\n",
    "\n",
    "> 《深度学习》 7.11 Bagging 和其他集成方法\n",
    "\n",
    "## 集成学习的基本策略\n",
    "\n",
    "### 1. Boosting\n",
    "\n",
    "- **Boosting**（提升）方法从某个**基学习器**出发，反复学习，得到一系列基学习器，然后组合它们构成一个强学习器。\n",
    "\n",
    "- Boosting 基于**串行策略**：基学习器之间存在依赖关系，新的学习器需要依据旧的学习器生成。\n",
    "\n",
    "- **代表算法/模型**：\n",
    "  - [提升方法 AdaBoost](#提升方法-adaboost)\n",
    "  - 提升树\n",
    "  - 梯度提升树 GBDT\n",
    "\n",
    "**Boosting 策略要解决的两个基本问题**\n",
    "\n",
    "1. 每一轮如何改变数据的权值或概率分布？\n",
    "2. 如何将弱分类器组合成一个强分类器？\n",
    "\n",
    "### 2. Bagging\n",
    "\n",
    "- Bagging 基于**并行策略**：基学习器之间不存在依赖关系，可同时生成。\n",
    "\n",
    "- **代表算法/模型**：\n",
    "  - [随机森林](#随机森林)\n",
    "  - 神经网络的 **Dropout** 策略\n",
    "\n",
    "## AdaBoost 算法\n",
    "\n",
    "AdaBoost，是英文\"Adaptive Boosting\"（自适应增强）的缩写。\n",
    "\n",
    "- [Adaboost算法原理分析和实例+代码（简明易懂）](https://blog.csdn.net/guyuealian/article/details/70995333) - CSDN\n",
    "\n",
    "## 前向分步算法\n",
    "\n",
    "\n",
    "## Reference\n",
    "\n",
    "- [【机器学习】集成学习(三)----前向分步算法、提升树与GBDT](https://blog.csdn.net/u013597931/article/details/79874439) - CSDN\n",
    "\n",
    "- [梯度提升树(GBDT)原理小结](https://www.cnblogs.com/pinard/p/6140514.html) - 刘建平Pinard\n",
    "\n",
    "- [GBDT（梯度提升决策树）](https://www.zybuluo.com/evilking/note/946535)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 梯度提升决策树 GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 随机森林\n",
    "\n",
    "随机森林的生成方法：\n",
    "\n",
    "1. 从样本集中通过重采样的方式产生 n 个样本\n",
    "2. 假设样本特征数目为 a，对 n 个样本选择 a 中的 k 个特征，用建立决策树的方式获得最佳分割点\n",
    "3. 重复 m 次，产生 m 棵决策树\n",
    "4. 多数投票机制来进行预测\n",
    "\n",
    "**（需要注意的一点是，这里 m 是指循环的次数，n 是指样本的数目，n 个样本构成训练的样本集，而 m 次循环中又会产生 m 个这样的样本集）**\n",
    "\n",
    "## Reference\n",
    "\n",
    "- [对于随机森林的通俗理解](https://blog.csdn.net/mao_xiao_feng/article/details/52728164) - CSDN\n",
    "\n",
    "- [随机森林算法学习(RandomForest)](https://blog.csdn.net/qq547276542/article/details/78304454) - CSDN\n",
    "\n",
    "- [机器学习中Bagging和Boosting的区别](https://blog.csdn.net/u013709270/article/details/72553282) - CSDN\n",
    "\n",
    "- [随机森林与决策树](https://clyyuanzi.gitbooks.io/julymlnotes/content/rf.html) - 机器学习笔记\n",
    "\n",
    "- []()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 降维\n",
    "\n",
    "## SVD\n",
    "\n",
    "## PCA\n",
    "\n",
    "## Reference\n",
    "\n",
    "- [降维方法PCA与SVD的联系与区别](https://www.cnblogs.com/bjwu/p/9280492.html) - 彎道超車\n",
    "\n",
    "- [PCA和SVD降维](https://blog.csdn.net/tianhaoyedl/article/details/77477568) - CSDN\n",
    "\n",
    "- [数据降维与可视化——t-SNE](https://blog.csdn.net/hustqb/article/details/78144384) - CSDN\n",
    "\n",
    "- [t-SNE实践——sklearn教程](https://blog.csdn.net/hustqb/article/details/80628721) - CSDN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reference\n",
    "\n",
    "- [使用集成学习提升机器学习算法性能](https://blog.csdn.net/u010099080/article/details/77720711) - CSDN\n",
    "\n",
    "- [快速理解bootstrap,bagging,boosting-三个概念](https://blog.csdn.net/wangqi880/article/details/49765673) - CSDN\n",
    "\n",
    "- []()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
