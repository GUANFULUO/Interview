{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. 逻辑斯蒂回归（Logistic Regression）\n",
    "\n",
    "\n",
    "## Reference\n",
    "\n",
    "- [Logistic Regression（逻辑回归）原理及公式推导](https://blog.csdn.net/programmer_wei/article/details/52072939) - CSDN\n",
    "\n",
    "- [逻辑回归推导](https://www.cnblogs.com/daguankele/p/6549891.html) - 罐装可乐 - 博客园\n",
    "\n",
    "- [极大似然估计详解](https://blog.csdn.net/zengxiantao1994/article/details/72787849) - CSDN\n",
    "\n",
    "- [一文搞懂极大似然估计](https://zhuanlan.zhihu.com/p/26614750) - 知乎\n",
    "\n",
    "- [梯度下降原理及Python实现](https://blog.csdn.net/programmer_wei/article/details/51941358) - CSDN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 支持向量机（SVM）\n",
    "\n",
    "SVM 和 Logistic 回归的比较：\n",
    "\n",
    "（1）经典的SVM，直接输出类别，不给出后验概率；\n",
    "\n",
    "（2）Logistic回归，会给出属于哪一个类别的后验概率；\n",
    "\n",
    "（3）比较重点是二者目标函数的异同。\n",
    "\n",
    "TODO\n",
    "\n",
    "- 凸二次优化\n",
    "- 拉格朗日乘子法\n",
    "\n",
    "## Reference\n",
    "\n",
    "- [SVM中支持向量的通俗解释](https://blog.csdn.net/AerisIceBear/article/details/79588583) - CSDN\n",
    "\n",
    "- [支持向量机SVM推导及求解过程](https://blog.csdn.net/american199062/article/details/51322852#commentBox) - CSDN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 决策树\n",
    "\n",
    "决策树的训练通常由三部分组成：\n",
    "\n",
    "- 特征选择\n",
    "- 树的生成\n",
    "- 剪枝\n",
    "\n",
    "## 信息增益与信息增益比\n",
    "\n",
    "TODO\n",
    "\n",
    "## 分类树 - ID3 决策树与 C4.5 决策树 \n",
    "\n",
    "ID3 决策树和 C4.5 决策树的区别在于：**前者使用信息增益来进行特征选择，而后者使用信息增益比。**\n",
    "\n",
    "TODO\n",
    "\n",
    "## 决策树如何避免过拟合\n",
    "\n",
    "TODO\n",
    "\n",
    "## 回归树 - CART 决策树\n",
    "\n",
    "> 《统计学习方法》 5.5 CART 算法\n",
    "\n",
    "- CART 算法是在给定输入随机变量 _`X`_ 条件下输出随机变量 _`Y`_ 的**条件概率分布**的学习方法。 \n",
    "- CART 算法假设决策树是**二叉树**，内部节点特征的取值为“**是**”和“**否**”。\n",
    "\n",
    "  这样的决策树等价于递归地二分每个特征，**将输入空间/特征空间划分为有限个单元**，然后在这些单元上确定在输入给定的条件下输出的**条件概率分布**。\n",
    "- CART 决策树**既可以用于分类，也可以用于回归**；\n",
    "\n",
    "对回归树 CART 算法用**平方误差最小化**准则来选择特征，对分类树用**基尼指数最小化**准则选择特征\n",
    "\n",
    "## Reference\n",
    "\n",
    "- []()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 集成学习\n",
    "\n",
    "- 基本思想：由多个学习器组合成一个性能更好的学习器\n",
    "- **集成学习为什么有效？**——不同的模型通常会在测试集上产生不同的误差。平均上，集成模型能至少与其任一成员表现一致；并且**如果成员的误差是独立的**，集成模型将显著地比其成员表现更好。\n",
    "\n",
    "> 《深度学习》 7.11 Bagging 和其他集成方法\n",
    "\n",
    "## 集成学习的基本策略\n",
    "\n",
    "### 1. Boosting\n",
    "\n",
    "\n",
    "### 2. Bagging\n",
    "\n",
    "\n",
    "## AdaBoost 算法\n",
    "\n",
    "## 前向分步算法\n",
    "\n",
    "\n",
    "## Reference\n",
    "\n",
    "- []()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 梯度提升决策树 GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
