## Index

<!-- TOC -->



<!-- /TOC -->

## 61. 准备机器学习面试应该了解哪些理论知识？

> [面试官如何判断面试者的机器学习水平？](https://www.zhihu.com/question/62482926) - 知乎

<img src="_asset/机器学习面试应该了解哪些理论知识.png">

看下来，这些问题的答案基本都在本BAT机器学习面试1000题系列里了。

## 62. 数据不平衡问题

这主要是由于数据分布不平衡造成的。解决方法如下：

- 采样，对小样本加噪声采样，对大样本进行下采样
- 数据生成，利用已知样本生成新的样本
- 进行特殊的加权，如在 Adaboost中 或者 SVM 中
- 采用对不平衡数据集不敏感的算法
- 改变评价标准：用 AUC/ROC 来进行评价
- 采用 Bagging/Boosting/ensemble 等方法
- 在设计模型的时候考虑数据的先验分布

## 63. 特征比数据量还大时，选择什么样的分类器？

> [机器学习面试题大全](http://blog.sina.com.cn/s/blog_178bcad000102x70r.html)

线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分。

## 64. 常见的分类算法有哪些？

SVM、神经网络、随机森林、逻辑回归、KNN、贝叶斯

## 65. 常见的监督学习算法有哪些？ 

感知机、svm、人工神经网络、决策树、逻辑回归

## 66. 说说常见的优化算法及其优缺点？

温馨提示：在回答面试官的问题的时候，往往将问题往大的方面去回答，这样不会陷于小的技术上死磕，最后很容易把自己嗑死了。

**简言之：**

1. **随机梯度下降**

  优点：可以一定程度上解决局部最优解的问题

  缺点：收敛速度较慢

2. **批量梯度下降**

  优点：容易陷入局部最优解

  缺点：收敛速度较快

3. **mini_batch梯度下降**

  综合随机梯度下降和批量梯度下降的优缺点，提取的一个中和的方法。

4. **牛顿法**

  牛顿法在迭代的时候，需要计算 Hessian 矩阵，当维度较高的时候，计算 Hessian 矩阵比较困难

5. **拟牛顿法**

  拟牛顿法是为了改进牛顿法在迭代过程中，计算 Hessian 矩阵而提取的算法，它采用的方式是通过逼近 Hessian 的方式来进行求解。

**具体而言：**

**从每个batch的数据来区分：**

- **梯度下降**：每次使用全部数据集进行训练
  - 优点：得到的是最优解
  - 缺点：运行速度慢，内存可能不够

- **随机梯度下降**：每次使用一个数据进行训练
  - 优点：训练速度快，无内存问题
  - 缺点：容易震荡，可能达不到最优解

- **Mini-batch**梯度下降
  - 优点：训练速度快，无内存问题，震荡较少
  - 缺点：可能达不到最优解

**从优化方法上来分：**

- **随机梯度下降（SGD）**

  - 缺点：

    选择合适的learning	rate比较难、

    对于所有的参数使用同样的learning rate	

    容易收敛到局部最优

    可能困在saddle point

- **SGD+Momentum**

  - 优点：

    积累动量，加速训练

    局部极值附近震荡时，由于动量，跳出陷阱

    梯度方向发生变化时，动量缓解动荡。

- **Nesterov Mementum**

  - 与Mementum类似，优点：

    避免前进太快

    提高灵敏度

- **AdaGrad**

  - 优点：

    控制学习率，每一个分量有各自不同的学习率

    适合稀疏数据

  - 缺点

    依赖一个全局学习率

    学习率设置太大，其影响过于敏感

    后期，调整学习率的分母积累的太大，导致学习率很低，提前结束训练。

- **RMSProp**

  - 优点：

    解决了后期提前结束的问题。

  - 缺点：

    依然依赖全局学习率

- **Adam**
  Adagrad 和 RMSProp的合体

  - 优点：

    结合了 Adagrad 善于处理稀疏梯度和 RMSprop 善于处理非平稳目标的优点

    为不同的参数计算不同的自适应学习率

    也适用于大多非凸优化 - 适用于大数据集和高维空间

- **牛顿法**

  牛顿法在迭代的时候，需要计算 Hessian 矩阵，当维度较高的时候，计算 Hessian 矩阵比较困难

- **拟牛顿法**

  拟牛顿法是为了改进牛顿法在迭代过程中，计算 Hessian 矩阵而提取的算法，它采用的方式是通过逼近 Hessian 的方式来进行求解。

## 67. 特征向量的归一化方法有哪些？

**线性函数转换，表达式如下：**

y=(x-MinValue)/(MaxValue-MinValue)

**对数函数转换，表达式如下：**

y=log10 (x)

**反余切函数转换 ，表达式如下：**

y=arctan(x)*2/PI

**减去均值，除以方差：**

y=(x-means)/ variance

## 68. RF与GBDT之间的区别与联系？

**相同点：**都是由多棵树组成，最终的结果都是由多棵树一起决定。

**不同点：**

- 组成随机森林的树可以分类树也可以是回归树，而 GBDT 只由回归树组成
- 组成随机森林的树可以并行生成，而 GBDT 是串行生成
- 随机森林的结果是多数表决表决的，而 GBDT 则是多棵树累加之和
- 随机森林对异常值不敏感，而 GBDT 对异常值比较敏感
- 随机森林是减少模型的方差，而 GBDT 是减少模型的偏差
- 随机森林不需要进行特征归一化。而 GBDT 则需要进行特征归一化

## 69. 试证明样本空间中任意点 `x` 到超平面 (w,b) 的距离为式（6.2）

<img src="_asset/证明题69.png">

## 70. 请比较下EM算法、HMM、CRF































