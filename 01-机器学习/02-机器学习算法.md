## 1. 机器学习算法选择

> [如何选择机器学习算法](https://zhuanlan.zhihu.com/p/26422223) - 知乎
>
> [Machine Learning:如何选择机器学习算法？](https://riboseyim.github.io/2018/04/02/Machine-Learning-Algorithms-Sheet/) - RiboseYim Blog
>
> [算法太多挑花眼？此文教你如何选择正确的机器学习算法](https://www.jiqizhixin.com/articles/choosing-the-right-machine-learning-algorithm) - 机器之心
>
> [8种常见算法比较](https://blog.csdn.net/qq_32425503/article/details/52164795) - CSDN

如何从各种各样的机器学习算法中选择解决自己感兴趣问题的方法，要解决该问题可以从以下几个因素来考虑：

- 数据的大小、质量及性质
- 可用计算时间
- 任务的急迫性
- 数据的使用用途



## Softmax Loss推一下

> [【技术综述】一文道尽softmax loss及其变种](https://zhuanlan.zhihu.com/p/34044634) - 知乎
>
> [卷积神经网络系列之softmax，softmax loss和cross entropy的讲解](https://blog.csdn.net/u014380165/article/details/77284921) - CSDN
>
> [详解softmax函数以及相关求导过程](https://zhuanlan.zhihu.com/p/25723112) - 知乎
>
> [Softmax损失函数及梯度的计算](https://zhuanlan.zhihu.com/p/21485970) - 知乎
>
> [简单易懂的softmax交叉熵损失函数求导](https://www.jianshu.com/p/c02a1fbffad6) - 简书
>
> [Softmax 输出及其反向传播推导](http://shuokay.com/2016/07/20/softmax-loss/) - Memo Blog



## Dropout 与 Bagging 集成方法的关系，Dropout 带来的意义与其强大的原因***

> [从bagging到dropout（deep learning笔记Ian）](https://blog.csdn.net/m0_37477175/article/details/77145459) - CSDN
>
> [最好的Dropout讲解](https://cloud.tencent.com/developer/article/1164228) - 腾讯云
>
> [理解dropout](https://blog.csdn.net/stdcoutzyx/article/details/49022443) - CSDN

**集成方法**：

其主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。这是机器学习中常规策略的一个例子，被称为**模型平均**（model averaging）。采用这种策略的技术被称为**集成方法**。

模型平均（model averaging）**奏效的原因**是不同的模型通常不会在测试集上产生完全相同的误差。平均上， 集成至少与它的任何成员表现得一样好，并且**如果成员的误差是独立的**，集成将显著地比其成员表现得更好。

**Bagging**：

Bagging（bootstrap aggregating）是通过结合几个模型降低泛化误差的技术 (Breiman, 1994)。

具体来说，Bagging 涉及构造 k 个**不同的数据集**。每个数据集从原始数据集中**重复采样**构成，和原始数据集具有**相同数量**的样例。这意味着，每个数据集以高概率缺少一些来自原始数据集的例子，还包含若干重复的例子（更具体的，如果采样所得的训练集与原始数据集大小相同，那所得数据集中大概有原始数据集 **2/3** 的实例）

**Dropout 的意义与强大的原因**：

简单来说，Dropout (Srivastava et al., 2014) 通过**参数共享**提供了一种廉价的 **Bagging** 集成近似，能够训练和评估**指数级数量**的神经网络。

Dropout 训练的集成包括所有从基础网络除去部分单元后形成的子网络。具体而言，只需将一些单元的**输出乘零**就能有效地删除一个单元。

通常，**隐藏层**的采样概率为 0.5，**输入**的采样概率为 0.8；超参数也可以采样，但其采样概率一般为 1

**Dropout与Bagging的不同点**：

- 在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型**共享参数**，其中每个模型继承父神经网络参数的不同子集。
- 在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。

**权重比例推断规则**：

简单来说，如果我们使用 0.5 的包含概率（keep prob），权重比例规则相当于在训练结束后**将权重除 2**，然后像平常一样使用模型；等价的，另一种方法是在训练期间将单元的状态乘 2。

无论哪种方式，我们的目标是确保在测试时一个单元的期望总输入与在训练时该单元的期望总输入是大致相同的（即使近半单位在训练时丢失）。

> 另一种深度学习算法——batch normalization,在训练时向隐藏单元引入加性和 乘性噪声重参数化模型。batch normalization的主要目的是改善优化,但噪音具有正 则化的效果,有时使Dropout变得没有必要。



## 推导 Backpropagation(BP反向传播算法)

> [零基础入门深度学习(3) - 神经网络和反向传播算法](https://www.zybuluo.com/hanbingtao/note/476663) - Blog
>
> [梯度下降与反向传播（含过程推导及证明）](https://blog.csdn.net/dugudaibo/article/details/77017485) - CSDN
>
> [如何直观地解释 backpropagation 算法？](https://www.zhihu.com/question/27239198) - 知乎
>
> [Neural Network中的Back-Propagation的详细推导过程](https://blog.csdn.net/wangzuhui0430/article/details/48967131) - CSDN
>
> [反向传播原理 & 卷积层backward实现](https://zhuanlan.zhihu.com/p/33802329) - 知乎
>
> [一文弄懂神经网络中的反向传播法——BackPropagation](https://www.cnblogs.com/charlotte77/p/5629865.html)

神经网络大多采取**正向传播预测，反向传播误差**的结构。反向传播算法是运用在神经网络中进行网络权重等最优值计算算法，其核心就是**梯度下降 + 链式法则求偏导**，虽然看起来很繁琐并且计算复杂度有点高，但是实际上BP算法的**精确性**和**易用性**是很难被其他算法替代的，这也是现在比如CNN等很火的深度学习算法普遍采用BP算法的原因。



# 1. 逻辑斯蒂回归（Logistic Regression）


## Reference

- [Logistic Regression（逻辑回归）原理及公式推导](https://blog.csdn.net/programmer_wei/article/details/52072939) - CSDN

- [逻辑回归推导](https://www.cnblogs.com/daguankele/p/6549891.html) - 罐装可乐 - 博客园

- [极大似然估计详解](https://blog.csdn.net/zengxiantao1994/article/details/72787849) - CSDN

- [一文搞懂极大似然估计](https://zhuanlan.zhihu.com/p/26614750) - 知乎

- [梯度下降原理及Python实现](https://blog.csdn.net/programmer_wei/article/details/51941358) - CSDN

# 2. 支持向量机（SVM）

SVM 和 Logistic 回归的比较：

（1）经典的SVM，直接输出类别，不给出后验概率；

（2）Logistic回归，会给出属于哪一个类别的后验概率；

（3）比较重点是二者目标函数的异同。

TODO

- 凸二次优化
- 拉格朗日乘子法

## Reference

- [SVM中支持向量的通俗解释](https://blog.csdn.net/AerisIceBear/article/details/79588583) - CSDN
- [支持向量机SVM推导及求解过程](https://blog.csdn.net/american199062/article/details/51322852#commentBox) - CSDN

# 3. 决策树

决策树的训练通常由三部分组成：

- 特征选择
- 树的生成
- 剪枝

## 信息增益与信息增益比

TODO

## 分类树 - ID3 决策树与 C4.5 决策树 

ID3 决策树和 C4.5 决策树的区别在于：**前者使用信息增益来进行特征选择，而后者使用信息增益比。**

TODO

## 决策树如何避免过拟合

TODO

## 回归树 - CART 决策树

> 《统计学习方法》 5.5 CART 算法

- CART 算法是在给定输入随机变量 _`X`_ 条件下输出随机变量 _`Y`_ 的**条件概率分布**的学习方法。 
- CART 算法假设决策树是**二叉树**，内部节点特征的取值为“**是**”和“**否**”。

  这样的决策树等价于递归地二分每个特征，**将输入空间/特征空间划分为有限个单元**，然后在这些单元上确定在输入给定的条件下输出的**条件概率分布**。
- CART 决策树**既可以用于分类，也可以用于回归**；

对回归树 CART 算法用**平方误差最小化**准则来选择特征，对分类树用**基尼指数最小化**准则选择特征

## Reference

- []()

# 4. 集成学习

- 基本思想：由多个学习器组合成一个性能更好的学习器
- **集成学习为什么有效？**——不同的模型通常会在测试集上产生不同的误差。平均上，集成模型能至少与其任一成员表现一致；并且**如果成员的误差是独立的**，集成模型将显著地比其成员表现更好。

> 《深度学习》 7.11 Bagging 和其他集成方法

## 集成学习的基本策略

### 1. Boosting

- **Boosting**（提升）方法从某个**基学习器**出发，反复学习，得到一系列基学习器，然后组合它们构成一个强学习器。

- Boosting 基于**串行策略**：基学习器之间存在依赖关系，新的学习器需要依据旧的学习器生成。

- **代表算法/模型**：
  - [提升方法 AdaBoost](#提升方法-adaboost)
  - 提升树
  - 梯度提升树 GBDT

**Boosting 策略要解决的两个基本问题**

1. 每一轮如何改变数据的权值或概率分布？
2. 如何将弱分类器组合成一个强分类器？

### 2. Bagging

- Bagging 基于**并行策略**：基学习器之间不存在依赖关系，可同时生成。

- **代表算法/模型**：
  - [随机森林](#随机森林)
  - 神经网络的 **Dropout** 策略

## AdaBoost 算法

AdaBoost，是英文"Adaptive Boosting"（自适应增强）的缩写。

- [Adaboost算法原理分析和实例+代码（简明易懂）](https://blog.csdn.net/guyuealian/article/details/70995333) - CSDN

## 前向分步算法


## Reference

- [【机器学习】集成学习(三)----前向分步算法、提升树与GBDT](https://blog.csdn.net/u013597931/article/details/79874439) - CSDN

- [梯度提升树(GBDT)原理小结](https://www.cnblogs.com/pinard/p/6140514.html) - 刘建平Pinard

- [GBDT（梯度提升决策树）](https://www.zybuluo.com/evilking/note/946535)

# 5. 梯度提升决策树 GBDT



# 6. 随机森林

随机森林的生成方法：

1. 从样本集中通过重采样的方式产生 n 个样本
2. 假设样本特征数目为 a，对 n 个样本选择 a 中的 k 个特征，用建立决策树的方式获得最佳分割点
3. 重复 m 次，产生 m 棵决策树
4. 多数投票机制来进行预测

**（需要注意的一点是，这里 m 是指循环的次数，n 是指样本的数目，n 个样本构成训练的样本集，而 m 次循环中又会产生 m 个这样的样本集）**

## Reference

- [对于随机森林的通俗理解](https://blog.csdn.net/mao_xiao_feng/article/details/52728164) - CSDN

- [随机森林算法学习(RandomForest)](https://blog.csdn.net/qq547276542/article/details/78304454) - CSDN

- [机器学习中Bagging和Boosting的区别](https://blog.csdn.net/u013709270/article/details/72553282) - CSDN

- [随机森林与决策树](https://clyyuanzi.gitbooks.io/julymlnotes/content/rf.html) - 机器学习笔记

- []()

# 7. 降维

## SVD

## PCA

## Reference

- [降维方法PCA与SVD的联系与区别](https://www.cnblogs.com/bjwu/p/9280492.html) - 彎道超車

- [PCA和SVD降维](https://blog.csdn.net/tianhaoyedl/article/details/77477568) - CSDN

- [数据降维与可视化——t-SNE](https://blog.csdn.net/hustqb/article/details/78144384) - CSDN

- [t-SNE实践——sklearn教程](https://blog.csdn.net/hustqb/article/details/80628721) - CSDN





## Reference

- [使用集成学习提升机器学习算法性能](https://blog.csdn.net/u010099080/article/details/77720711) - CSDN

- [快速理解bootstrap,bagging,boosting-三个概念](https://blog.csdn.net/wangqi880/article/details/49765673) - CSDN

- []()