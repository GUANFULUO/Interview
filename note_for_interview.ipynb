{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 贝壳找房深度学习\n",
    "\n",
    "## 1. 卷积层和全连接层的区别\n",
    "\n",
    "- 卷积层为局部连接；而全连接层则使用图像的全局信息。可以想象一下，最大的局部是不是就等于全局了？这首先说明全连接层使用卷积层来替代的可行性。\n",
    "\n",
    "- 全连接层的权重矩阵是固定的，卷积层就不需要固定大小了，因为它只是对局部区域进行窗口滑动，所以用卷积层取代全连接层成为了可能\n",
    "\n",
    "## 2. FCN与CNN的区别\n",
    "\n",
    "FCN的最后几层不是全连接层，而CNN是全连接层。这些全连接层都是一维的信息，丢失了二维的信息。\n",
    "CNN是图像到结果的网络，从二维信息到一维信息，一般输出一个图片的结果。而FCN网络是从图像到图像的网络，从二维信息到二维信息，是一个像素级的网络，对应每个像素点的结果。\n",
    "\n",
    "> CNN的识别是图像级的识别，也就是从图像到结果，而FCN的识别是像素级的识别，对输入图像的每一个像素在输出上都有对应的判断标注，标明这个像素最可能是属于一个什么物体/类别。CNN的识别是图像级的识别，也就是从图像到结果，而FCN的识别是像素级的识别，对输入图像的每一个像素在输出上都有对应的判断标注，标明这个像素最可能是属于一个什么物体/类别。\n",
    "\n",
    "**上采样和下采样**\n",
    "\n",
    "CNN从高维度到低维度，卷积+池化的过程就是下采样。FCN从低维度到高维度，反卷积的过程就是上采样。\n",
    "\n",
    "bilinear interpolation 双线性内插\n",
    "\n",
    "**感受野**\n",
    "\n",
    "> 这里就涉及到一个感受野（receptive field）的概念。较浅的卷积层（靠前的）的感受域比较小，学习感知细节部分的能力强，较深的隐藏层(靠后的)，感受域相对较大，适合学习较为整体的，相对更宏观一些的特征。\n",
    "\n",
    "感受野被定义为特定 CNN 特征正在“看”（即受其影响）的输入空间中的区域。特征的感受野可以通过其中心位置及其大小进行充分描述。\n",
    "\n",
    "所以在做反卷积的时候，会考虑浅层的卷积信息，辅助叠加得到更好的分割结果。\n",
    "\n",
    "\n",
    "## 3. L0 L1 L2\n",
    "\n",
    "<a href=\"https://www.codecogs.com/eqnedit.php?latex=L_0\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?L_0\" title=\"L_0\" /></a>：向量中非零向量的个数\n",
    "\n",
    "<a href=\"https://www.codecogs.com/eqnedit.php?latex=L_1\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?L_1\" title=\"L_1\" /></a>：向量中所有元素的绝对值之和\n",
    "\n",
    "<a href=\"https://www.codecogs.com/eqnedit.php?latex=|x|_1&space;=&space;\\sum_{i}|x_i|\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?|x|_1&space;=&space;\\sum_{i}|x_i|\" title=\"|x|_1 = \\sum_{i}|x_i|\" /></a>\n",
    "\n",
    "<a href=\"https://www.codecogs.com/eqnedit.php?latex=L_2\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?L_2\" title=\"L_2\" /></a>：向量中所有元素的平方和的开方\n",
    "\n",
    "<a href=\"https://www.codecogs.com/eqnedit.php?latex=||x||_2&space;=&space;\\sqrt{\\sum_{i}|x_i|^2}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?||x||_2&space;=&space;\\sqrt{\\sum_{i}|x_i|^2}\" title=\"||x||_2 = \\sqrt{\\sum_{i}|x_i|^2}\" /></a>\n",
    "\n",
    "其中 L1 和 L2 范数分别是 Lp (p>=1) 范数的特例：\n",
    "\n",
    "<a href=\"https://www.codecogs.com/eqnedit.php?latex=||x||_p&space;=&space;(\\sum_{i}|x_i|^p)^&space;\\frac{1}{p}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?||x||_p&space;=&space;(\\sum_{i}|x_i|^p)^&space;\\frac{1}{p}\" title=\"||x||_p = (\\sum_{i}|x_i|^p)^ \\frac{1}{p}\" /></a>\n",
    "\n",
    "**范数的应用**：\n",
    "\n",
    "- 正则化——权重衰减/参数范数惩罚\n",
    "\n",
    "**权重衰减的目的**\n",
    "\n",
    "- 限制模型的学习能力，通过限制参数 θ 的规模（主要是权重 w 的规模，偏置 b 不参与惩罚），使模型偏好于权值较小的目标函数，防止过拟合。\n",
    "\n",
    "### 3.1 L1 和 L2 范数的异同***\n",
    "\n",
    "**相同点**\n",
    "\n",
    "- 限制模型的学习能力，通过限制参数的规模，使模型偏好于权值较小的目标函数，防止过拟合。\n",
    "\n",
    "**不同点**\n",
    "\n",
    "- L1 正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择；一定程度上防止过拟合\n",
    "- L2 正则化主要用于防止模型过拟合\n",
    "- L1 适用于特征之间有关联的情况；L2 适用于特征之间没有关联的情况\n",
    "\n",
    "> [机器学习中正则化项L1和L2的直观理解](https://blog.csdn.net/jinping_shi/article/details/52433975) - CSDN博客\n",
    "\n",
    "## 4. 防止过拟合方法\n",
    "- 数据增强方法\n",
    "- python中tuple和list的区别\n",
    "- 复杂度了解吗，二分查找复杂度\n",
    "- 快排最好情况的复杂度\n",
    "- 手撕代码：二分查找\n",
    "- 了解堆和栈\n",
    "- 用栈结构实现O(1)时间复杂度找到栈内的最大元素，如果有很多重复元素时怎么改进\n",
    "- PCA原理\n",
    "- 特征值和特征向量的含义\n",
    "- 机器学习理论+数学基础+数据结构+coding+项目\n",
    "- SGD 中 S(stochastic)代表什么\n",
    "- 监督学习／迁移学习／半监督学习／弱监督学习／非监督学习？\n",
    "- Softmax Loss推一下\n",
    "- CNN的特点以及优势 \n",
    "- 推导 backward\n",
    "- 解释dropout以及实现机制\n",
    "- 深度学习中有什么加快收敛/降低训练难度的方法\n",
    "- 什么造成过拟合，如何防止过拟合\n",
    "\n",
    "## Reference\n",
    "\n",
    "- [深度学习---之卷积层与全连接层的区别](https://blog.csdn.net/zxyhhjs2017/article/details/78605283) - CSDN\n",
    "\n",
    "- [为什么使用卷积层替代CNN末尾的全连接层](http://www.voidcn.com/article/p-zcfjydks-bqs.html) - 程序圆\n",
    "\n",
    "- [图像的上采样（upsampling）与下采样（subsampled）](https://blog.csdn.net/majinlei121/article/details/46742339) - CSDN\n",
    "\n",
    "- [浅谈L0,L1,L2范数及其应用](http://t.hengwei.me/post/%E6%B5%85%E8%B0%88l0l1l2%E8%8C%83%E6%95%B0%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8.html) - Blog\n",
    "\n",
    "- [机器学习中的范数规则化之（一）L0、L1与L2范数](https://blog.csdn.net/zouxy09/article/details/24971995) - CSDN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 便利蜂\n",
    "\n",
    "- 特征工程做的有哪些？非线性可分的情况怎么处理的？\n",
    "- SVM的核函数了解多少？\n",
    "- L1与L2区别？L1为啥具有稀疏性？\n",
    "- xgboost的原理\n",
    "- sigmoid函数的导函数的取值范围是多少？其实就是一元二次方程的y值范围，0-1/4\n",
    "\n",
    "- Python中协成的概念，即微线程，具体可以看廖雪峰的网站\n",
    "- C++中vector增删改的时间复杂度，O(1)，O(n)，O(n)\n",
    "- MySQL中索引用的什么数据结构？B-Tree或B+Tree\n",
    "- Hash_table的底层是什么实现的？拉链法，数组+链表\n",
    "- HBase的列式存储解释\n",
    "\n",
    "## 链家\n",
    "\n",
    "- 最小二乘与极大似然函数的关系？从概率统计的角度处理线性回归并在似然概率为高斯函数的假设下同最小二乘简历了联系\n",
    "- LR为啥是个线性模型？本质就是线性的，只是特征到结果映射用的是sigmoid函数，或者说回归边界是线性的，即P(Y=1|x)=P(Y=0|x)时有`W*x=0`\n",
    "- 分类的评价标准，准确度，AUC，召回率等等\n",
    "- 有的逻辑回归损失函数中为啥要加 `-1*m`\n",
    "- 欠拟合的解决方法？模型简单，加深神经网络，svm用核函数等等\n",
    "- L2正则的本质？限制解空间范围，缩小解空间，控制模型复杂度\n",
    "- SVM引入核函数本质？提高维度，增加模型复杂度\n",
    "\n",
    "## 滴滴\n",
    "\n",
    "- 介绍xgboost、gbdt、rf的区别\n",
    "- 树模型的特征选择中除了信息增益、信息增益比、基尼指数这三个外，还有哪些？\n",
    "- Sklearn中树模型输出的特征重要程度是本身的还是百分比？\n",
    "- 介绍下SVM以及它的核函数\n",
    "- 熟悉FM算法不\n",
    "- 算法题：两个链表的第一个公共节点\n",
    "\n",
    "- 进程和线程的区别？\n",
    "- HBase数据库的优点？\n",
    "\n",
    "## xx\n",
    "\n",
    "- 算法题：两个数字链表求和，将结果也存到一个链表里面，注意相加超10时进位就行\n",
    "- RF与xgboost的区别？怎样选取的特征？如何判断这些特征的重要程度？最后RF的层数和深度是多少？\n",
    "- 还用了深层神经网络？几层？用GPU没？特征维度到底多少？服务器配置？啥？你能把全部数据放进内存？\n",
    "\n",
    "## 乐信\n",
    "\n",
    "- 关联规则中，置信度和支持度的概念？\n",
    "- MySQL中MYISAM和InnoDB的区别\n",
    "- LR，svm，rf等算法的区别\n",
    "- 模型评价指标，解释AUC，准确率和召回率\n",
    "- 对于同一个数据，怎样根据AUC判断模型的好坏？数据？\n",
    "\n",
    "## 新浪门户\n",
    "\n",
    "- 介绍LR，为啥用的是似然函数不用最小二乘？当用lr时，特征中的某些值很大，意味着这个特征重要程度很高？对吗？不对，用lr时需要对特征进行离散化。。。\n",
    "- L1和L2正则的区别？\n",
    "- 树模型中，特征选择方法有哪些？ID3和C4.5分裂后，节点的信息熵是变大还是变小？变小\n",
    "- RF和gbdt的区别\n",
    "- 介绍下深度学习，CNN中的卷积和池化\n",
    "- Hadoop中shuffle过程\n",
    "\n",
    "## 58到家\n",
    "\n",
    "- 了解贝叶斯不？它的应用场景都有哪些？\n",
    "- 知道哪些深度学习的框架？\n",
    "- 网络分几层？TCP和UDP区别？写个快排吧\n",
    "- 场景题：一个10T的文本，一个10M的文本，从大文本中找出与小文本中相似度大于80%的文本，提示，用SameHash\n",
    "- 场景题：北京市所有小区的客户发出家政请求的可能性(回归问题)；或者从家政的全部业务流程中找出一个具体场景进行分析：家政阿姨接到派单通知后，进行家政服务的路径选择，可阿姨一天顶多服务2-3个家庭，该如何派单？\n",
    "\n",
    "## 百度\n",
    "\n",
    "- 怎样用的KNN进行的预测\n",
    "- XGBoost与RF的区别\n",
    "- RF的随机性体现在哪里？它的代码中输出的特征重要程度是怎么进行计算的？\n",
    "- 实习项目中的评价标准是什么？accuracy和precision、recall这些一样吗？AUC的解释\n",
    "- 了解哪些损失函数？区别是啥？\n",
    "- 线性模型为何用的最小二乘作为损失函数而不用似然函数或者交叉熵？\n",
    "- 了解哪些深度学习模型？keras底层用TensorFlow和theano时，代码有何不同？TensorFlow原理、流程图，session是啥？\n",
    "- 编程题：两个数组的最长公共子序列和最长递增子序列，用DP写出来后\n",
    "\n",
    "## 陌陌\n",
    "\n",
    "- LR与SVM的区别\n",
    "- GBDT与XGBoost的区别？\n",
    "- 了解FM吗？GBDT的数据在使用前有什么需要注意的吗？\n",
    "- 做过广告点击率预估没？LR+GBDT和GBDT+FM怎么结合的知道不？\n",
    "\n",
    "- LR与GBDT的结合了解不\n",
    "- 智力算法题(说是它的校招笔试题)：\n",
    "    f(x)=p,y=0; 1-p,y=1 将这个概率函数转换为T(x)=1/2, y=0或者y=1\n",
    "    提示：f(x)执行四次可能出现的结果有0,0：p^2 0,1:p(-1p) 1,0:p(1-p) 1,1(1-p)^2\n",
    "    里面有两次结果的概率是一样的\n",
    "    \n",
    "## 美团\n",
    "\n",
    "- 数据特征怎样选择的？怎样表示的？模型的选择？当时的模型参数是多少？结果效果如何？\n",
    "- RF与GBDT的区别？为啥你要用集成的方法而不用准确度更高的算法模型？\n",
    "- 推导LR\n",
    "- 编程题：删除链表中倒数第K个节点，写出来后，面试官加难度：如果是个带环的链表呢？也就是先找到环的入口，再注意边界条件就行\n",
    "\n",
    "- 为啥LR的输入特征一般是离散的而不是连续的？\n",
    "- 了解各种优化算法不？梯度下降和随机梯度下降的区别？牛顿法和拟牛顿法的区别？为啥提出拟牛顿？因为牛顿法涉及海塞矩阵，它的逆矩阵求解很麻烦\n",
    "- KNN的使用场景\n",
    "- 智力题：\n",
    "    1<=a,b<=99，甲手里有a+b的结果，乙手里有a*b的值，两人目前都不知道a和b的值，两人对话如下：\n",
    "    1甲：你肯定不知道a，b的值是多少\n",
    "    2乙：我好像知道了\n",
    "    3甲：我好像也知道了\n",
    "    问：a，b的值是多少\n",
    "    思路：对话1说明a+b的可能性有多种组合，其对应的乘积也有多种组合，形成两个集合S和T\n",
    "    对话2说明乙根据手里的乘积结果，可以得出S和T这两个集合的交集是唯一的\n",
    "    对话3说明甲猜到了乙的想法，故也能猜出交集唯一。。。\n",
    "\n",
    "## 凤凰网\n",
    "\n",
    "- TCP和UDP的区别？\n",
    "- 编程题：数组A[N+1]中每个数据的都是1<=x<=N，其中只有一个数字是重复的，请找出来。使用一些技巧性的方法，比如根据值找对应索引位置，将其变为负数，依次这样，知道发现要变的那个已经是负数，找到了。。。\n",
    "- 解释rf，xgboost，gbdt的区别\n",
    "- 编程题：从数组A中找出所有和为S的两个数的索引，leetcode 原题\n",
    "\n",
    "- 顺时针打印矩阵：剑指offer上的原题"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
