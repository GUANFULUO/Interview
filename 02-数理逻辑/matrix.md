> [矩阵的转置的意义是什么?](https://www.zhihu.com/question/38372986) - 知乎
>
> [掌握机器学习数学基础之线代（重点知识）](https://zhuanlan.zhihu.com/p/30191876) - 知乎
>
> [深度学习中的数学与技巧(8):矩阵及其变换、特征值与特征向量的物理意义](https://blog.csdn.net/u011534057/article/details/52872736) - CSDN
>
> [不懂这些线性代数知识 别说你是搞机器学习的](https://www.jianshu.com/p/a92b785afef2) - 简书
>
> [线性代数A矩阵乘以A的转置的含义或者几何意义](https://blog.csdn.net/yewei11/article/details/49982591) - CSDN
>
> [对称矩阵对角化的意义何在？？](https://www.zhihu.com/question/27831523) - 知乎

# 矩阵转置的意义

转置的作用：「把左边的 x 映射到右边再与右边的 y 求内积」，可以与「把右边的 y 映射到左边再与左边的 x 求内积」相互替换，这两种操作中使用的映射，表示成矩阵时互为转置关系。

（下面以A(T)表示A的转置.）

先从奇异值说起.我个人的理解,奇异值是特征值的一种推广.因为只有方阵才可能具有特征值,对于实际遇到的一些问题（比如最小二乘问题）,往往遇上长方阵,长方阵根本没有特征值.因而就有必要对特征值做推广,这就是奇异值.

再看什么是奇异值.对于任意矩阵A（甚至是非方的）,A(T)A（这个时候就变成方阵了,可以算特征值了）的特征值就称为A的奇异值.奇异值有个特性,就是A(T)A和AA(T)特征值相同.

特征值和奇异值的关系.对于长方阵来说,它根本不存在特征值,所以之后再讨论.对于方阵来说,容易证明,其所有奇异值恰好为其所有特征值的模长的平方（即奇异值全实非负）,因而奇异值和特征值有相当良好的对应关系.

奇异值为什么重要.我们知道,对于一个方阵来说,特征分解后,从特征值和特征向量我们就可以知道矩阵的大量性质.对于非方阵来说,我们也希望得到一个这样信息量巨大的分解,这就是奇异值分解（SVD）.

最后看一下SVD分解和最小二乘的关系.我们知道,最小二乘有个解法,对于Ax = b的最小二乘问题,等价于求解其法方程A(T)Ax = A(T)b,这个时候就变成方阵的问题了.但是这种算法是不稳定的.一种更为有效的算法就是SVD分解并利用广义逆求解.

看一下广义逆和最小二乘、SVD的关系.广义逆可以百度一下.定义有很多式子.但是,对于可逆阵来说,广义逆就是逆.这里把A的广义逆记作A(+).则Ax = b的最小二乘解就是x = A(+)b.所以,现在的问题就是,怎么求A的广义逆A(+).通过SVD分解,



## 特征值和特征向量的几何和物理意义

> [特征值和特征向量的几何和物理意义](https://blog.csdn.net/dongtinghong/article/details/14216139)
>
> []()





