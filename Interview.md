# 贝壳找房深度学习

## 1. 卷积层和全连接层的区别

- 卷积层为局部连接；而全连接层则使用图像的全局信息。可以想象一下，最大的局部是不是就等于全局了？这首先说明全连接层使用卷积层来替代的可行性。
- 全连接层的权重矩阵是固定的，卷积层就不需要固定大小了，因为它只是对局部区域进行窗口滑动，所以用卷积层取代全连接层成为了可能

> - [深度学习---之卷积层与全连接层的区别](https://blog.csdn.net/zxyhhjs2017/article/details/78605283) - CSDN

## 2. FCN与CNN的区别

FCN的最后几层不是全连接层，而CNN是全连接层。这些全连接层都是一维的信息，丢失了二维的信息。
CNN是图像到结果的网络，从二维信息到一维信息，一般输出一个图片的结果。而FCN网络是从图像到图像的网络，从二维信息到二维信息，是一个像素级的网络，对应每个像素点的结果。

> CNN的识别是图像级的识别，也就是从图像到结果，而FCN的识别是像素级的识别，对输入图像的每一个像素在输出上都有对应的判断标注，标明这个像素最可能是属于一个什么物体/类别。CNN的识别是图像级的识别，也就是从图像到结果，而FCN的识别是像素级的识别，对输入图像的每一个像素在输出上都有对应的判断标注，标明这个像素最可能是属于一个什么物体/类别。

**上采样和下采样**

CNN从高维度到低维度，卷积+池化的过程就是下采样。FCN从低维度到高维度，反卷积的过程就是上采样。

bilinear interpolation 双线性内插

**感受野**

> 这里就涉及到一个感受野（receptive field）的概念。较浅的卷积层（靠前的）的感受域比较小，学习感知细节部分的能力强，较深的隐藏层(靠后的)，感受域相对较大，适合学习较为整体的，相对更宏观一些的特征。

感受野被定义为特定 CNN 特征正在“看”（即受其影响）的输入空间中的区域。特征的感受野可以通过其中心位置及其大小进行充分描述。

所以在做反卷积的时候，会考虑浅层的卷积信息，辅助叠加得到更好的分割结果。

> - [为什么使用卷积层替代CNN末尾的全连接层](http://www.voidcn.com/article/p-zcfjydks-bqs.html) - 程序圆
> - [图像的上采样（upsampling）与下采样（subsampled）](https://blog.csdn.net/majinlei121/article/details/46742339) - CSDN


## 3. L0 L1 L2

<a href="https://www.codecogs.com/eqnedit.php?latex=L_0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?L_0" title="L_0" /></a>：向量中非零向量的个数

<a href="https://www.codecogs.com/eqnedit.php?latex=L_1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?L_1" title="L_1" /></a>：向量中所有元素的绝对值之和

<a href="https://www.codecogs.com/eqnedit.php?latex=|x|_1&space;=&space;\sum_{i}|x_i|" target="_blank"><img src="https://latex.codecogs.com/gif.latex?|x|_1&space;=&space;\sum_{i}|x_i|" title="|x|_1 = \sum_{i}|x_i|" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=L_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?L_2" title="L_2" /></a>：向量中所有元素的平方和的开方

<a href="https://www.codecogs.com/eqnedit.php?latex=||x||_2&space;=&space;\sqrt{\sum_{i}|x_i|^2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?||x||_2&space;=&space;\sqrt{\sum_{i}|x_i|^2}" title="||x||_2 = \sqrt{\sum_{i}|x_i|^2}" /></a>

其中 L1 和 L2 范数分别是 Lp (p>=1) 范数的特例：

<a href="https://www.codecogs.com/eqnedit.php?latex=||x||_p&space;=&space;(\sum_{i}|x_i|^p)^&space;\frac{1}{p}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?||x||_p&space;=&space;(\sum_{i}|x_i|^p)^&space;\frac{1}{p}" title="||x||_p = (\sum_{i}|x_i|^p)^ \frac{1}{p}" /></a>

**范数的应用**：

- 正则化——权重衰减/参数范数惩罚

**权重衰减的目的**

- 限制模型的学习能力，通过限制参数 θ 的规模（主要是权重 w 的规模，偏置 b 不参与惩罚），使模型偏好于权值较小的目标函数，防止过拟合。

### 3.1 L1 和 L2 范数的异同***

**相同点**

- 限制模型的学习能力，通过限制参数的规模，使模型偏好于权值较小的目标函数，防止过拟合。

**不同点**

- L1 正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择；一定程度上防止过拟合
- L2 正则化主要用于防止模型过拟合
- L1 适用于特征之间有关联的情况；L2 适用于特征之间没有关联的情况

> - [机器学习中正则化项L1和L2的直观理解](https://blog.csdn.net/jinping_shi/article/details/52433975) - CSDN博客
>
> - [浅谈L0,L1,L2范数及其应用](http://t.hengwei.me/post/%E6%B5%85%E8%B0%88l0l1l2%E8%8C%83%E6%95%B0%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8.html) - Blog
> - [机器学习中的范数规则化之（一）L0、L1与L2范数](https://blog.csdn.net/zouxy09/article/details/24971995) - CSDN

## 4. 防止过拟合方法

- 参数范数惩罚（Parameter Norm Penalties）
- 数据增强（Dataset Augmentation）
- 提前终止（Early Stopping）
- 参数绑定与参数共享（Parameter Tying and Parameter Sharing）
- Bagging 和其他集成方法
- Dropout
- 批标准化（Batch Normalization）

## 5. 数据增强方法

常用的数据增强方法有：

- 旋转 | 反射变换(Rotation/reflection): 随机旋转图像一定角度; 改变图像内容的朝向;
- 翻转变换(flip): 沿着水平或者垂直方向翻转图像;
- 缩放变换(zoom): 按照一定的比例放大或者缩小图像;
- 平移变换(shift): 在图像平面上对图像以一定方式进行平移;
- 可以采用随机或人为定义的方式指定平移范围和平移步长, 沿水平或竖直方向进行平移. 改变图像内容的位置;
- 尺度变换(scale): 对图像按照指定的尺度因子, 进行放大或缩小; 或者参照SIFT特征提取思想, 利用指定的尺度因子对图像滤波构造尺度空间. 改变图像内容的大小或模糊程度;
- 对比度变换(contrast): 在图像的HSV颜色空间，改变饱和度S和V亮度分量，保持色调H不变. 对每个像素的S和V分量进行指数运算(指数因子在0.25到4之间), 增加光照变化;
- 噪声扰动(noise): 对图像的每个像素RGB进行随机扰动, 常用的噪声模式是椒盐噪声和高斯噪声;
- 颜色变化：在图像通道上添加随机扰动。
- 输入图像随机选择一块区域涂黑，参考《Random Erasing Data Augmentation》

> - [数据增强方法总结](https://blog.csdn.net/Iriving_shu/article/details/78762567) - CSDN

## 6. python中tuple和list的区别

- set：无序，不重复，可修改
  - 把set理解key的集合，更合适，因为set中存在的就是不可变对象
- list：有序，可重复，可修改
- tuple：有序，可重复，不可修改

## 7. 复杂度了解吗，二分查找复杂度

二分查找的基本思想是将n个元素分成大致相等的两部分，去a[n/2]与x做比较，如果x=a[n/2],则找到x,算法中止；如果x<a[n/2],则只要在数组a的左半部分继续搜索x,如果x>a[n/2],则只要在数组a的右半部搜索x.

时间复杂度无非就是while循环的次数！

总共有n个元素，

渐渐跟下去就是n,n/2,n/4,....n/2^k，其中k就是循环的次数

由于你n/2^k取整后>=1

即令n/2^k=1

可得k=log2n,（是以2为底，n的对数）

所以时间复杂度可以表示O()=O(logn)

## 8. 快排最好情况的复杂度

最好情况的复杂度：O(nlgn) 

最坏情况的复杂度：O(n^2)

> - [快速排序最好，最坏，平均复杂度分析](https://blog.csdn.net/weshjiness/article/details/8660583) - CSDN

## 9. 手撕代码：二分查找

二分查找又叫折半查找，二分查找应该属于减治技术的成功应用。所谓减治法，就是将原问题分解成若干个子问题后，利用了规模为n的原问题的解与较小规模（通常是n/2）的子问题的解之间的关系。 

二分查找利用了记录按关键码有序的特点，其基本思想为：在有序表中，取中间记录作为比较对象，若给定值与中间记录的关键码相等，则查找成功；若给定值小于中间记录的关键码，则在中间记录的左半边继续查找；若给定值大于中间记录的关键码，则在中间记录右半边区继续查找。不断重复上述过程，直到查找成功，或所查找的区域无记录，查找失败。 

二分查找的时间复杂度是O(log(n))，最坏情况下的时间复杂度是O(n)。

例如： 
   在有序列表list1中[1, 3, 8, 12, 23, 31, 37, 42, 48, 58]中查找值为8的记录的。

伪代码：

```reStructuredText
1.low = 0; high = len(list1]-1 #设置初识查找区间
2.测试查找区间[low, high]是否存在，若不存在，则查找失败；否则
3.取中间mid=(low + high)/2；比较k与list1[mid]，有以下三种情况：
  3.1 若k<r[mid]，则high=mid-1；查找在左半区进行，转2；
  3.2 若k>r[mid]，则low=mid+1；查找在右半边区进行，转2；
  3.3 若k=r[mid]，则查找成功，返回记录在表中位置mid；
```

Python实现二分查找算法，代码如下：

```python
def binary_search(alist, item):
    """
    :type item: int
    :type alist: list
    :rtype bool
    """
	n = len(alist)
    start = 0
    end = n - 1
    while start <= end:
        mid = (start + end) // 2
        if alist[mid] == item:
            return True
        elif alist[mid] > item:
            end = mid -1
        else:
            start = mid + 1
    
    return False

# 递归方式实现
def binary_search(alist, item):
    n = len(alist)
    if 0 == n:
        return False
    mid = n // 2
    if alist[mid] == item:
        return True
    elif alist[mid] > item:
        return binary_search(alist[:mid], item)
    else:
        return binary_search(alist[mid+1:], item)
```

最优时间复杂度：O(1) 
最坏时间复杂度：O(logn)

## 10. 了解堆和栈

一个由C/C++编译的程序占用的内存分为以下几个部分

1、栈区（stack）— 由编译器自动分配释放 ，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。

2、堆区（heap） — 一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS回收 。注意它与数据结构中的堆是两回事，分配方式倒是类似于链表。

3、全局区（静态区）（static）—，全局变量和静态变量的存储是放在一块的，初始化的全局变量和静态变量在一块区域， 未初始化的全局变量和未初始化的静态变量在相邻的另一块区域。 - 程序结束后有 
 系统释放

4、文字常量区 —常量字符串就是放在这里的。 程序结束后由系统释放

5、程序代码区—存放函数体的二进制代码。

**堆栈缓存方式**

- 栈使用的是一级缓存， 他们通常都是被调用时处于存储空间中，调用完毕立即释放。存取速度较快。
- 堆则是存放在二级缓存中，生命周期由虚拟机的垃圾回收算法来决定（并不是一旦成为孤儿对象就能被回收）。所以调用这些对象的速度要相对来得低一些。

通常函数参数，返回值，局部变量，已知变量类型（int、float等）分配在栈上

自己new、malloc、alloc或者自定义类，系统不知道多大的分配在堆上

值类型是分配在栈里面，引用类型分配在堆里面。 

<img src="source/堆栈.png">

**Python 内存管理机制，总结非常详细：**

> - [Python内存管理机制***](https://www.cnblogs.com/geaozhang/p/7111961.html)
> - [了解堆和栈](https://blog.csdn.net/shagua_nan/article/details/50916130)
> - [python有没有堆和栈的概念？](https://www.zhihu.com/question/34835069)

## 11. 用栈结构实现O(1)时间复杂度找到栈内的最小元素，如果有很多重复元素时怎么改进

由于栈具有后进先出（Last In First Out, LIFO）的特点，因此 push 和 pop 只需要对栈顶元素进行操作，只能访问到栈顶元素，而无法得到栈中最小的元素。

总体思想是以空间来换时间，设置两个栈，其中一个栈放元素，一个栈放最小的值，在push时，判断新push进去的元素和原先栈中的元素哪个小，小的才有资格入栈；在pop元素时候，要记得将存放最小值的栈的栈顶元素也pop出来。很简单。栈的实现用链表的实现方式。

**Python 中的数据结构与算法：**`*****`

> - [Python_数据结构与算法***](https://segmentfault.com/a/1190000014141614)

## 12. PCA原理

> 在多元统计分析中，**主成分分析**（**Principal components analysis**，**PCA**）是一种分析、简化数据集的技术。主成分分析经常用于减少数据集的维数，同时保持数据集中的对方差贡献最大的特征。这是通过保留低阶主成分，忽略高阶主成分做到的。这样低阶成分往往能够保留住数据的最重要方面。

PCA在机器学习中经常被用到，是数据预处理的重要步骤。它主要基于以下考虑：

- 高维特征中很多特征之间存在相关性，含有冗余信息
- 相比于低维数据，高维数据计算更复杂

PCA的原理就是将原来的样本数据投影到一个新的空间中，相当于我们在矩阵分析里面学习的将一组矩阵映射到另外的坐标系下。通过一个转换坐标，也可以理解成把一组坐标转换到另外一组坐标系下，但是在新的坐标系下，表示原来的原本不需要那么多的变量，只需要原来样本的**最大的一个线性无关组的特征值对应的空间的坐标即可**。

**一般来说，PCA降维后的每个样本的特征的维数，不会超过训练样本的个数**，因为超出的特征是没有意义的。

**降维的必要性：**

1. 多重共线性--预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。

2. 高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有0.02%。

3. 过多的变量会妨碍查找规律的建立。

4. 仅在变量层面上分析可能会忽略变量之间的潜在联系。例如几个预测变量可能落入仅反映数据某一方面特征的一个组内。

**降维的目的：**

1. 减少预测变量的个数

2. 确保这些变量是相互独立的

3. 提供一个框架来解释结果

降维的方法有：主成分分析、因子分析、用户自定义复合等。

**PCA算法**

将原始数据按列组成n行m列矩阵X

1. 将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值。
2. 求出协方差矩阵C=1mXXTC=1mXXT
3. 求出协方差矩阵的特征值及对应的特征向量
4. 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P
5. Y=PX即为降维到k维后的数据

**PCA算法总结**

这里对PCA算法做一个总结。作为一个非监督学习的降维方法，它只需要特征值分解，就可以对数据进行压缩，去噪。因此在实际场景应用很广泛。为了克服PCA的一些缺点，出现了很多PCA的变种，比如第六节的为解决非线性降维的KPCA，还有解决内存限制的增量PCA方法Incremental PCA，以及解决稀疏数据降维的PCA方法Sparse PCA等。

PCA算法的主要优点有：

- 仅仅需要以方差衡量信息量，不受数据集以外的因素影响。　

- 各主成分之间正交，可消除原始数据成分间的相互影响的因素。

- 计算方法简单，主要运算是特征值分解，易于实现。

PCA算法的主要缺点有：

- 主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。

- 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。

> **正常的PCA应该是协方差矩阵求特征值和特征向量**

> * [主成分分析（PCA）原理详解](https://zhuanlan.zhihu.com/p/37777074) - 知乎
> * [PCA的数学原理及推导证明](https://zhuanlan.zhihu.com/p/26951643) - 知乎
> * [主成分分析PCA工作原理和简单介绍](https://blog.csdn.net/suibianshen2012/article/details/51353484) - CSDN
> * [主成分分析（PCA）原理详解](https://blog.csdn.net/zhongkelee/article/details/44064401) - CSDN
> * [『 特征降维』PCA原理-Principal Component Analysis](https://blog.csdn.net/shine19930820/article/details/71037209) - CSDN
> * [主成分分析（PCA）原理总结](https://www.cnblogs.com/pinard/p/6239403.html) - 刘建平Pinard

## 13. 特征值和特征向量的含义

> - [特征值和特征向量](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E5%80%BC%E5%92%8C%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F) - 维基百科
>
> 在[数学](https://zh.wikipedia.org/wiki/%E6%95%B0%E5%AD%A6)上，特别是[线性代数](https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0)中，对于一个给定的矩阵{\displaystyle A}![A](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3)，它的**特征向量**（eigenvector，也译**固有向量**或**本征向量**）{\displaystyle v}![v](https://wikimedia.org/api/rest_v1/media/math/render/svg/e07b00e7fc0847fbd16391c778d65bc25c452597) 经过这个线性变换[[1\]](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E5%80%BC%E5%92%8C%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F#endnote_1transfo)之后，得到的新向量仍然与原来的{\displaystyle v}![v](https://wikimedia.org/api/rest_v1/media/math/render/svg/e07b00e7fc0847fbd16391c778d65bc25c452597) 保持在同一条[直线](https://zh.wikipedia.org/wiki/%E7%9B%B4%E7%B7%9A)上，但其[长度](https://zh.wikipedia.org/wiki/%E9%95%BF%E5%BA%A6)或方向也许会改变。即
>
> ![{\displaystyle Av=\lambda v}](https://wikimedia.org/api/rest_v1/media/math/render/svg/9f7a927ca654271e6607c7f9cded57fea70c1d71)，
>
> ![\lambda ](https://wikimedia.org/api/rest_v1/media/math/render/svg/b43d0ea3c9c025af1be9128e62a18fa74bedda2a)为[标量](https://zh.wikipedia.org/wiki/%E7%B4%94%E9%87%8F)，即特征向量的长度在该线性变换下缩放的比例，称{\displaystyle \lambda }![\lambda ](https://wikimedia.org/api/rest_v1/media/math/render/svg/b43d0ea3c9c025af1be9128e62a18fa74bedda2a) 为其**特征值**（本征值）。如果特征值为正，则表示{\displaystyle v}![v](https://wikimedia.org/api/rest_v1/media/math/render/svg/e07b00e7fc0847fbd16391c778d65bc25c452597) 在经过线性变换的作用后方向也不变；如果特征值为负，说明方向会反转；如果特征值为0，则是表示缩回零点。但无论怎样，仍在同一条直线上。图1给出了一个以著名油画《[蒙娜丽莎](https://zh.wikipedia.org/wiki/%E8%92%99%E5%A8%9C%E4%B8%BD%E8%8E%8E)》为题材的例子。在一定条件下（如其矩阵形式为实对称矩阵的线性变换），一个变换可以由其特征值和特征向量完全表述，也就是说：所有的特征向量组成了这向量空间的一组[基底](https://zh.wikipedia.org/wiki/%E5%9F%BA%E5%BA%95)。一个**特征空间**(eigenspace)是具有相同特征值的特征向量与一个同维数的零向量的集合，可以证明该集合是一个[线性子空间](https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E5%AD%90%E7%A9%BA%E9%97%B4)，比如{\displaystyle \textstyle E_{\lambda }=\{u\in V\mid Au=\lambda u\}}![{\displaystyle \textstyle E_{\lambda }=\{u\in V\mid Au=\lambda u\}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e4d44061b752adffc2036fe5476e95c2dfbd9a5f) 即为线性变换{\displaystyle A}![A](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3) 中以{\displaystyle \lambda }![\lambda ](https://wikimedia.org/api/rest_v1/media/math/render/svg/b43d0ea3c9c025af1be9128e62a18fa74bedda2a) 为特征值的**特征空间**。

> **特征值和特征向量的意义就在于此!**
>
> **矩阵所充当的映射,实际上就是对特征向量的缩放,每个特征向量的缩放程度就是特征值.**
>
> - 特征值就是运动的速度
> - 特征向量就是运动的方向
> - 特征值就是拉伸的大小
> - 特征向量指明了拉伸的方向

> **矩阵的乘法，本质是一种运动。**

<img src="source/特征值和特征向量.png">

<img src="source/特征空间.png">

对于方阵而言，矩阵不会进行维度的升降，所以矩阵代表的运动实际上只有两种：

- 旋转
- 拉伸

> 特征值只有方阵才可能有 因为要想Ax=tx t是特征值 则必然要求A的列数=x的行数=A的行数
>
> 而条件数就不一样了 A的条件数被定义为A的范数乘以A的广义逆的范数 非方阵也是有可能有广义逆的 其广义逆是(A的共轭转置乘以A)的逆乘以A的共轭转置 只要A是行满秩或者列满秩的 其广义逆就存在 从而有条件数
>
> 对于2范数意义下的的条件数 其值为A的最大奇异值除以A的最小奇异值

> - [矩阵乘法的本质是什么？***](https://www.zhihu.com/question/21351965) - 知乎
> - [如何理解矩阵特征值？***](https://www.zhihu.com/question/21874816/answer/181864044) - 知乎
> - [特征值（eigenvalue）特征向量（eigenvector）特征值分解（eigenvalue decomposition）](https://blog.csdn.net/zhengwei223/article/details/78913898) - CSDN

## 14. SGD 中 S(stochastic)代表什么

**直觉上结论是：相对于非随机算法，SGD 能更有效的利用信息，特别是信息比较冗余的时候。**

**实验上结论是：相对于非随机算法， SGD 在前期迭代效果卓越。**

**理论上结论是：如果样本数量大，那么 SGD的计算复杂度依然有优势。**

> - [为什么我们更宠爱“随机”梯度下降？（SGD）](https://zhuanlan.zhihu.com/p/28060786) - 知乎
> - [从动力学角度看随机梯度下降：一些小启示](https://zhuanlan.zhihu.com/p/39354316) - 知乎
>
> - [深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）](https://zhuanlan.zhihu.com/p/22252270) - 知乎

## 15. 数据预处理的方法

> 数据预处理和特征选择是数据挖掘与机器学习中关注的重要问题，坊间常说：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。特征工程就是将原始数据转化为有用的特征，更好的表示预测模型处理的实际问题，提升对于未知数据的预测准确性。

### 15.1 数据挖掘中使用的数据的原则

- 尽可能赋予属性名和属性值明确的含义；
- 去除惟一属性；
- 去除重复性；
- 合理选择关联字段。

### 15.2 常见的数据预处理方法

- 数据清洗：数据清洗的目的不只是要消除错误、冗余和数据噪音，还要能将按不同的、不兼容的规则所得的各种数据集一致起来。
- 数据集成：将多个数据源中的数据合并，并存放到一个一致的数据存储（如数据仓库）中。这些数据源可能包括多个数据库、数据立方体或一般文件。
- 数据变换：找到数据的特征表示，用维度变换来减少有效变量的数目或找到数据的不变式，包括规格化、规约、切换和投影等操作。
- 数据规约：是在对发现任务和数据本身内容理解的基础上，寻找依赖于发现目标的表达数据的有用特征，以缩减数据模型，从而在尽可能保持数据原貌的前提下最大限度的精简数据量，主要有两个途径：属性选择和数据抽样，分别针对数据库中的属性和记录。

> - [数据预处理与特征选择](https://blog.csdn.net/u010089444/article/details/70053104) - CSDN
> - [机器学习-常见的数据预处理](https://blog.csdn.net/yehui_qy/article/details/53791006) - CSDN
> - [机器学习——字典学习/稀疏编码学习笔记](https://zhuanlan.zhihu.com/p/26015351)

## 16. 稀疏矩阵

### 16.1 稀疏的问题

稀疏矩阵会导致空间复杂度和时间复杂度的问题。

**空间复杂度**
非常大的矩阵需要大量的内存，而我们想要处理的一些非常大的矩阵是稀疏的。

> *在实践中，大多数大型矩阵都是稀疏的——几乎所有的项都为零。*

—第465页，《线性代数介绍》（Introduction to Linear Algebra），第五版，2016年。

一个非常大的矩阵的例子是，因为它太大而不能存储在内存中，这是一个显示从一个网站到另一个网站的链接的链接矩阵。一个更小的稀疏矩阵的例子可能是一个单词或术语的出现矩阵，在一本书中与所有已知的英语单词对应。

在这两种情况下，所包含的矩阵都是稀疏的，其零值比数据值要多。将这些稀疏矩阵表示为稠密矩阵的问题是对内存的要求，并且必须为矩阵中的每个32位或64位零值做出分配。

这显然是对内存资源的浪费，因为这些零值不包含任何信息。

**时间复杂度**
假设一个非常大的稀疏矩阵可以适应内存，我们将需要对这个矩阵执行操作。

简单地说，如果矩阵包含了大部分零值，也就是没有数据，那么在这个矩阵中执行操作可能需要很长时间，其中的大部分计算都需要或将零值相加或相乘。

> *在这样的问题上使用线性代数的一般方法是很浪费的，因为大多数O(N^3)算术运算都用于求解方程组或反转（invert）包含零操作数的矩阵。*

—第75页，《数值分析：科学计算的艺术》（Numerical Recipes: The Art of Scientific Computing），第三版，2007年。

这是矩阵运算的时间复杂度增加的问题，随着矩阵的大小而增加。

当我们考虑到即使是琐碎的机器学习方法可能需要对每一行、列甚至整个矩阵进行许多操作时，这个问题也会变得更加复杂，从而导致执行时间大大延长。

### 16.2 机器学习中的稀疏矩阵

稀疏矩阵在应用机器学习中经常出现。

在这一节中，我们将讨论一些常见的例子，以激发你对稀疏问题的认识。

**数据**
稀疏矩阵在某些特定类型的数据中出现，最值得注意的是记录活动的发生或计数的观察。

三个例子包括:

- 用户是否在一个电影目录中有曾经看过的电影。
- 用户是否在一个产品目录中有已经购买过的产品。
- 在一个歌曲目录中数出收听过的歌曲的数量。

**数据准备**
在准备数据时，稀疏矩阵会出现在编码方案中。

三种常见的例子包括:

- 独热编码，用来表示分类数据为稀疏的二进制向量。
- 计数编码，用于表示文档中词汇的频率。
- TF-IDF编码，用于表示词汇中标准化的单词频率得分。

**领域研究**
机器学习中的一些领域必须开发专门的方法来解决稀疏问题，因为输入的数据几乎总是稀疏的。

三个例子包括:

- 用于处理文本文档的自然语言处理。
- 推荐系统在一个目录中进行产品使用。
- 当处理图像时计算机视觉包含许多黑色像素（black pixel）。

> *如果在语言模型中有100,000个单词，那么特征向量长度为100,000，但是对于一个简短的电子邮件来说，几乎所有的特征都是0。*

—第22页，《人工智能：一种现代方法》（Artificial Intelligence: A Modern Approach），第三版，2009年。

### 16.3 处理稀疏矩阵

表示和处理稀疏矩阵的解决方案是使用另一个数据结构来表示稀疏数据。

零值可以被忽略，只有在稀疏矩阵中的数据或非零值需要被存储或执行。

多个数据结构可以用来有效地构造一个稀疏矩阵;下面列出了三个常见的例子。

- Dictionary of Keys。在将行和列索引映射到值时使用字典。
- List of Lists。矩阵的每一行存储为一个列表，每个子列表包含列索引和值。
- Coordinate List。一个元组的列表存储在每个元组中，其中包含行索引、列索引和值。

还有一些更适合执行高效操作的数据结构;下面列出了两个常用的示例。

- 压缩的稀疏行。稀疏矩阵用三个一维数组表示非零值、行的范围和列索引。
- 压缩的稀疏列。与压缩的稀疏行方法相同，除了列索引外，在行索引之前被压缩和读取。

被压缩的稀疏行，也称为CSR，通常被用来表示机器学习中的稀疏矩阵，因为它支持的是有效的访问和矩阵乘法。

> - [机器学习稀疏矩阵简介(附Python代码)](https://zhuanlan.zhihu.com/p/35032245) - 知乎
> - [一篇关于机器学习中的稀疏矩阵的介绍](https://zhuanlan.zhihu.com/p/34534763) - 知乎

### 16.4 推荐系统

> 嵌入层甚至可以用来处理推荐系统中的稀疏矩阵问题。

> - [深度学习系列 4: 为什么你需要使用嵌入层](https://juejin.im/post/599183c6f265da3e2e5717d2) - 掘金

### 16.5 [推荐系统]欧氏距离和余弦相似度

> **前者是看成坐标系中两个点，来计算两点之间的距离；**
>
> **后者是看成坐标系中两个向量，来计算两向量之间的夹角。**

> 余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似，夹角等于0，即两个向量相等，这就叫"余弦相似性"。

> - [欧氏距离和余弦相似度](https://blog.csdn.net/SunnyYoona/article/details/39721485) - CSDN
> - [余弦计算相似度度量](https://blog.csdn.net/SunnyYoona/article/details/39721205) - CSDN

## 17. 特征提取的方法和过程

> - 特征选择是指去掉无关特征，保留相关特征的过程，也可以认为是从所有的特征中选择一个最好的特征子集。特征选择本质上可以认为是降维的过程。
>
> - 特征提取是指将机器学习算法不能识别的原始数据转化为算法可以识别的特征的过程。比如说，文本是由一系列文字组成的，这些文字在经过分词后会形成一个词语集合，对于这些词语集合（原始数据），机器学习算法是不能直接使用的，我们需要将它们转化成机器学习算法可以识别的数值特征（固定长度的向量表示），然后再交给机器学习的算法进行操作。再比如说，图片是由一系列像素点构（原始数据）成的，这些像素点本身无法被机器学习算法直接使用，但是如果将这些像素点转化成矩阵的形式（数值特征），那么机器学习算法就可以使用了。

模式识别的大致流程如下：

<img src="source/模式识别.png">

特征提取与选择是在分类器设计之前完成，它主要的工作是针对数据原始特征的缺陷，降低特征维数，提高分类器的设计与性能。

> - [机器学习系列：（三）特征提取与处理](https://blog.csdn.net/u013719780/article/details/51743867) - CSDN
> - [图像特征提取三大法宝：HOG特征，LBP特征，Haar特征](http://dataunion.org/20584.html) - 数盟
> - [特征选择与特征提取](https://blog.csdn.net/henryczj/article/details/41284201) - CSDN

## 18. Python numpy，scipy，pandas这些库的区别是什么？

- Numpy是以矩阵为基础的数学计算模块，纯数学。

- Scipy基于Numpy，科学计算库，有一些高阶抽象和物理模型。比方说做个傅立叶变换，这是纯数学的，用Numpy；做个滤波器，这属于信号处理模型了，在Scipy里找。

- Pandas提供了一套名为DataFrame的数据结构，比较契合统计分析中的表结构，并且提供了计算接口，可用Numpy或其它方式进行计算。

> - NumPy：N维数组容器
>
> - SciPy：科学计算函数库
>
> - Pandas：表格容器

## 19. 机器学习算法选择

如何从各种各样的机器学习算法中选择解决自己感兴趣问题的方法，要解决该问题可以从以下几个因素来考虑：

- 数据的大小、质量及性质
- 可用计算时间
- 任务的急迫性
- 数据的使用用途

> - [如何选择机器学习算法](https://zhuanlan.zhihu.com/p/26422223) - 知乎
> - [Machine Learning:如何选择机器学习算法？](https://riboseyim.github.io/2018/04/02/Machine-Learning-Algorithms-Sheet/) - RiboseYim Blog
> - [算法太多挑花眼？此文教你如何选择正确的机器学习算法](https://www.jiqizhixin.com/articles/choosing-the-right-machine-learning-algorithm) - 机器之心
> - [8种常见算法比较](https://blog.csdn.net/qq_32425503/article/details/52164795) - CSDN

## 20. 监督学习／迁移学习／半监督学习／弱监督学习／非监督学习？

### 20.1 机器学习算法的类型

**监督学习（Supervised Learning）**

监督学习是基于样本集进行预测的算法。例如历史的销售额可以用来预测未来商品的价格。监督学习算法拥有一个由标记训练样本构成的输入变量和一个期望的输出变量。采用算法分析训练数据能够学习出输入到输出的映射函数，通过训练样本到期望结果的泛化预测新的未知样本的输出结果：

- **分类（Classification）**：如果数据用来预测一个类别变量，那么监督学习算法又称作分类，比如对一张图片标注标签（狗或者猫）。如果只有两个标签，则称作二分类问题（binary classification），当类别大于两个时，称作多类别分类问题（multi-class classification）
- **回归（Regression）**：当预测变量为连续变量时，问题就转化为一个回归问题
- **预测（Forecasting）**：一种基于过去和现在数据预测未来的处理过程，主要用于分析事物发展的趋势。例如，基于过去几年和今年的销售额估计明年的销售额问题

**半监督学习（Semi-supervised Learning）**

监督学习一个主要问题是有标签数据的获取非常耗时耗力。如果标记数据有限，可以采用未标记样本增强监督学习的性能。由于算法并不是完全监督的，所以称作半监督算法。半监督学习算法可以利用未标记样本和少量标记样本提升学习算法精度。

**非监督学习（Unsupervised Learning）**

非监督学习算法完全使用未标记数据，挖掘数据潜在的固有模式，例如聚类结构、低维流形或者稀疏树和图。

- 聚类（Clustering）：对样本数据进行分组，使得同一分组（类别）下的数据比不同分组的数据更相似，聚类算法常用于将整个数据集划分为多个组，然后对每个组内的数据进行分析，有助于找到数据内存的模式。
- 降维（Dimension Reduction）：减少可能使用的变量数目。在很多实际应用中，原始数据可能维度很高，而且特征之间存在冗余性或与任务无关，这种情况下降维有助于找到变量之间真实的潜在关系。

**强化学习（Reinforcement Learning）**

强化学习基于环境中智能体的反馈分析并优化其行为，算法在不同场景下进行测试从而发现能够产生最高奖励的动作，而不是被动地选择动作。试验与误差和延迟奖励是强化学习区别其他算法的特性。

### 20.2 迁移学习

> 迁移学习(Transfer learning) 顾名思义就是就是把已学训练好的模型参数迁移到新的模型来帮助新模型训练。考虑到大部分数据或任务是存在相关性的，所以通过迁移学习我们可以将已经学到的模型参数（也可理解为模型学到的知识）通过某种方式来分享给新模型从而加快并优化模型的学习效率不用像大多数网络那样从零学习（starting from scratch，tabula rasa）。

> - [深度学习 -> 强化学习 ->迁移学习（杨强教授报告）***)](https://blog.csdn.net/jiandanjinxin/article/details/54133521) - CSDN
> - [什么是迁移学习 (Transfer Learning)？这个领域历史发展前景如何？***](https://www.zhihu.com/question/41979241/answer/208177153)
> - [独家：一文读懂迁移学习（附学习工具包）](http://www.xtecher.com/Xfeature/view?aid=7383) - THU数据派

## 21. Softmax Loss推一下



> - [【技术综述】一文道尽softmax loss及其变种](https://zhuanlan.zhihu.com/p/34044634) - 知乎
> - [卷积神经网络系列之softmax，softmax loss和cross entropy的讲解](https://blog.csdn.net/u014380165/article/details/77284921) - CSDN
> - [详解softmax函数以及相关求导过程](https://zhuanlan.zhihu.com/p/25723112) - 知乎
> - [Softmax损失函数及梯度的计算](https://zhuanlan.zhihu.com/p/21485970) - 知乎
> - [简单易懂的softmax交叉熵损失函数求导](https://www.jianshu.com/p/c02a1fbffad6) - 简书
> - [Softmax 输出及其反向传播推导](http://shuokay.com/2016/07/20/softmax-loss/) - Memo Blog

## 22. CNN的特点以及优势 


CNN使用范围是具有局部空间相关性的数据，比如图像，自然语言，语音

- 局部感知 ： 通过在底层的局部扫描获得图像的局部特征，然后在高层综合这些特征获取图像的全局信息。作用：降低参数的数目。

- 权值共享  ： CNN中每个卷积核里面的参数即权值，原始图片卷积后会得到一副新的图片，而新图中的每个像素都来自同一个卷积核，这就是权值共享。作用：进一步降低参数数目。

- 多卷积核   ： 一个卷积核只能学习到图像的某一种特征，而设置多个卷积核时，则可以学到多种特征。每个卷积核也是通过学习得到的，那么会不会多个卷积核学到的特征相同呢！这就涉及到权值初始化的问题了！

- 降维    ： 卷积层中设置stride本身就有降维的效果，但大多是通过池化层中的max-pooling方法进行特征压缩，提取主要特征！

- 多层次结构：将低层次的局部特征组合成为较高层次的特征。不同层级的特征可以对应不同任务。

---

1. 卷积：对图像元素的矩阵变换，是提取图像特征的方法，多种卷积核可以提取多种特征。一个卷积核覆盖的原始图像的范围叫做感受野（权值共享）。一次卷积运算(哪怕是多个卷积核)提取的特征往往是局部的，难以提取出比较全局的特征，因此需要在一层卷积基础上继续做卷积计算 ，这也就是多层卷积。

2. 池化：降维的方法，按照卷积计算得出的特征向量维度大的惊人，不但会带来非常大的计算量，而且容易出现过拟合，解决过拟合的办法就是让模型尽量“泛化”，也就是再“模糊”一点，那么一种方法就是把图像中局部区域的特征做一个平滑压缩处理，这源于局部图像一些特征的相似性(即局部相关性原理)。
3. 全连接：softmax分类
   训练过程：
   卷积核中的因子(×1或×0)其实就是需要学习的参数，也就是卷积核矩阵元素的值就是参数值。一个特征如果有9个值，1000个特征就有900个值，再加上多个层，需要学习的参数还是比较多的。
   CNN的三个优点：

CNN的三个优点：

- sparse interaction(稀疏的交互)，
- parameter sharing(参数共享)，
- equivalent respresentation(等价表示)。适合于自动问答系统中的答案选择模型的训练。

> - [面试常问的深度学习(DNN、CNN、RNN)的相关问题](https://blog.csdn.net/xwd18280820053/article/details/76026523) - CSDN
> - [卷积神经网络CNN总结](https://www.cnblogs.com/skyfsm/p/6790245.html) - Blog


## 23. 推导 Backpropagation(BP反向传播算法)

神经网络大多采取**正向传播预测，反向传播误差**的结构。反向传播算法是运用在神经网络中进行网络权重等最优值计算算法，其核心就是**梯度下降 + 链式法则求偏导**，虽然看起来很繁琐并且计算复杂度有点高，但是实际上BP算法的**精确性**和**易用性**是很难被其他算法替代的，这也是现在比如CNN等很火的深度学习算法普遍采用BP算法的原因。



> - [零基础入门深度学习(3) - 神经网络和反向传播算法](https://www.zybuluo.com/hanbingtao/note/476663) - Blog
> - [梯度下降与反向传播（含过程推导及证明）](https://blog.csdn.net/dugudaibo/article/details/77017485) - CSDN
> - [如何直观地解释 backpropagation 算法？](https://www.zhihu.com/question/27239198) - 知乎
> - [Neural Network中的Back-Propagation的详细推导过程](https://blog.csdn.net/wangzuhui0430/article/details/48967131) - CSDN
> - [反向传播原理 & 卷积层backward实现](https://zhuanlan.zhihu.com/p/33802329) - 知乎
> - [一文弄懂神经网络中的反向传播法——BackPropagation](https://www.cnblogs.com/charlotte77/p/5629865.html)

## 24. Dropout 与 Bagging 集成方法的关系，Dropout 带来的意义与其强大的原因***

**集成方法**：

其主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。这是机器学习中常规策略的一个例子，被称为**模型平均**（model averaging）。采用这种策略的技术被称为**集成方法**。

模型平均（model averaging）**奏效的原因**是不同的模型通常不会在测试集上产生完全相同的误差。平均上， 集成至少与它的任何成员表现得一样好，并且**如果成员的误差是独立的**，集成将显著地比其成员表现得更好。

**Bagging**：

Bagging（bootstrap aggregating）是通过结合几个模型降低泛化误差的技术 (Breiman, 1994)。

具体来说，Bagging 涉及构造 k 个**不同的数据集**。每个数据集从原始数据集中**重复采样**构成，和原始数据集具有**相同数量**的样例。这意味着，每个数据集以高概率缺少一些来自原始数据集的例子，还包含若干重复的例子（更具体的，如果采样所得的训练集与原始数据集大小相同，那所得数据集中大概有原始数据集 **2/3** 的实例）

**Dropout 的意义与强大的原因**：

简单来说，Dropout (Srivastava et al., 2014) 通过**参数共享**提供了一种廉价的 **Bagging** 集成近似，能够训练和评估**指数级数量**的神经网络。

Dropout 训练的集成包括所有从基础网络除去部分单元后形成的子网络。具体而言，只需将一些单元的**输出乘零**就能有效地删除一个单元。

通常，**隐藏层**的采样概率为 0.5，**输入**的采样概率为 0.8；超参数也可以采样，但其采样概率一般为 1

**Dropout与Bagging的不同点**：

- 在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型**共享参数**，其中每个模型继承父神经网络参数的不同子集。
- 在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。

**权重比例推断规则**：

简单来说，如果我们使用 0.5 的包含概率（keep prob），权重比例规则相当于在训练结束后**将权重除 2**，然后像平常一样使用模型；等价的，另一种方法是在训练期间将单元的状态乘 2。

无论哪种方式，我们的目标是确保在测试时一个单元的期望总输入与在训练时该单元的期望总输入是大致相同的（即使近半单位在训练时丢失）。

>  另一种深度学习算法——batch normalization,在训练时向隐藏单元引入加性和 乘性噪声重参数化模型。batch normalization的主要目的是改善优化,但噪音具有正 则化的效果,有时使Dropout变得没有必要。

> - [从bagging到dropout（deep learning笔记Ian）](https://blog.csdn.net/m0_37477175/article/details/77145459) - CSDN
> - [最好的Dropout讲解](https://cloud.tencent.com/developer/article/1164228) - 腾讯云
> - [理解dropout](https://blog.csdn.net/stdcoutzyx/article/details/49022443) - CSDN

## 25. 深度学习中有什么加快收敛/降低训练难度的方法

- 瓶颈结构
- 残差
- 学习率、步长、动量
- 优化方法
- 预训练

> - [深度学习模型训练痛点及解决方法](https://www.cnblogs.com/carsonzhu/p/9547852.html) - 博客园


## 26. 什么造成过拟合，如何防止过拟合

### 26.1 过拟合的一些解决方案***

- 参数范数惩罚（Parameter Norm Penalties）
- 数据增强（Dataset Augmentation）
- 提前终止（Early Stopping）
- 参数绑定与参数共享（Parameter Tying and Parameter Sharing）
- Bagging 和其他集成方法
- Dropout
- 批标准化（Batch Normalization）

### 26.2 训练误差、泛化误差；过拟合、欠拟合；模型容量，表示容量，有效容量，最优容量的概念； 奥卡姆剃刀原则*

**“奥卡姆剃刀”三法**

　　对于组织在[目标设置](https://wiki.mbalib.com/wiki/%E7%9B%AE%E6%A0%87%E8%AE%BE%E7%BD%AE)与执行过程中因上述种种原因而出现的目标曲解与置换，有一个根本的解决之道，即“无情地剔除所有累赘”，这也正是“奥卡姆剃刀”所倡导的“简化”法则：保持事物的简单化是对付复杂与繁琐的最有效方式。具体而言，有三种措施可以帮助我们避免目标曲解与置换现象的发生：

　　**1、精兵简政，不断简化组织结构**

　　[组织结构扁平化](https://wiki.mbalib.com/wiki/%E7%BB%84%E7%BB%87%E7%BB%93%E6%9E%84%E6%89%81%E5%B9%B3%E5%8C%96)与组织结构非层级化已经成为企业[组织变革](https://wiki.mbalib.com/wiki/%E7%BB%84%E7%BB%87%E5%8F%98%E9%9D%A9)的基本趋势。在新型的组织结构中，传统的企业组织结构中严格的[等级制度](https://wiki.mbalib.com/wiki/%E7%AD%89%E7%BA%A7%E5%88%B6%E5%BA%A6)已经不复存在，组织中上下有序的传统规则被淡化，员工之间的关系是平等的分工合作关系，基层员工被赋予更多的权力，他们有可能参与部门目标甚至于[组织目标](https://wiki.mbalib.com/wiki/%E7%BB%84%E7%BB%87%E7%9B%AE%E6%A0%87)的制定，组织内的信息不再是上下级之间的单向传递，而是一种网络化的即时式[双向沟通](https://wiki.mbalib.com/wiki/%E5%8F%8C%E5%90%91%E6%B2%9F%E9%80%9A)。在这种组织中，顾客的需要成为员工行动的向导，人们的行为具有明确的目标导向。同时，由于员工的积极参与，组织目标与个人目标之间的矛盾得到最大程度地消除。

　　**2、关注组织的核心价值，始终将组织资源集中于自己的专长**

　　也就是说，组织需要从众多可供选择的业务中筛选出最重要的、拥有[核心竞争能力](https://wiki.mbalib.com/wiki/%E6%A0%B8%E5%BF%83%E7%AB%9E%E4%BA%89%E8%83%BD%E5%8A%9B)的业务，在自己最具竞争优势的领域确定组织的目标。这样，才能确保组织集中精力，就可以以最少的代价获得最丰厚的[利润](https://wiki.mbalib.com/wiki/%E5%88%A9%E6%B6%A6)。反之，如果目标数量过多，往往会使经营者难以同时兼顾太多的业务，从而顾此失彼。[韦尔奇](https://wiki.mbalib.com/wiki/%E9%9F%A6%E5%B0%94%E5%A5%87)上任[通用电气公司](https://wiki.mbalib.com/wiki/%E9%80%9A%E7%94%A8%E7%94%B5%E6%B0%94%E5%85%AC%E5%8F%B8)总裁时，从简洁高效的角度出发，提出[“非一即二”原则](https://wiki.mbalib.com/wiki/%E2%80%9C%E9%9D%9E%E4%B8%80%E5%8D%B3%E4%BA%8C%E2%80%9D%E5%8E%9F%E5%88%99)：必须把本产品做成数一数二的产品，否则一律卖掉。

　　**3、简化流程，避免不必要的文书作业**

　　事实上，由于个体受自身[思维方式](https://wiki.mbalib.com/wiki/%E6%80%9D%E7%BB%B4%E6%96%B9%E5%BC%8F)的限制，简单的信息远比复杂的信息更有利于人们的思考与[决策](https://wiki.mbalib.com/wiki/%E5%86%B3%E7%AD%96)。因此一个优秀企业的主要特征，就是他们知道如何保持事情的简单化，不管多复杂的事情都能将其变得简单易行。

　　尽管导致组织目标曲解与置换的原因很多，但奥卡姆剃刀定律对解决目标的曲解与置换为我们提供了一种“简单”的理念与思路。

> 奥卡姆剃刀定律，是由14世纪逻辑学家、圣方济各会修士奥卡姆的威廉（William of Occam，约1285年至1349年）提出。这个原理称为“如无必要，勿增实体”，即“简单有效原理”。
>
> 应用：
>
> ①做选择时，添加必要条件，提出非必要条件，然后去计算概率，每一项选择的结果出现的概率。

> - [罗胖说的最重要的“奥卡姆剃刀原则”是什么？](https://www.jianshu.com/p/8102cb404a45) - 简书

# 便利蜂

- 特征工程做的有哪些？非线性可分的情况怎么处理的？
- SVM的核函数了解多少？
- L1与L2区别？L1为啥具有稀疏性？
- xgboost的原理
- sigmoid函数的导函数的取值范围是多少？其实就是一元二次方程的y值范围，0-1/4

- Python中协成的概念，即微线程，具体可以看廖雪峰的网站
- C++中vector增删改的时间复杂度，O(1)，O(n)，O(n)
- MySQL中索引用的什么数据结构？B-Tree或B+Tree
- Hash_table的底层是什么实现的？拉链法，数组+链表
- HBase的列式存储解释

# 链家

- 最小二乘与极大似然函数的关系？从概率统计的角度处理线性回归并在似然概率为高斯函数的假设下同最小二乘简历了联系
- LR为啥是个线性模型？本质就是线性的，只是特征到结果映射用的是sigmoid函数，或者说回归边界是线性的，即P(Y=1|x)=P(Y=0|x)时有`W*x=0`
- 分类的评价标准，准确度，AUC，召回率等等
- 有的逻辑回归损失函数中为啥要加 `-1*m`
- 欠拟合的解决方法？模型简单，加深神经网络，svm用核函数等等
- L2正则的本质？限制解空间范围，缩小解空间，控制模型复杂度
- SVM引入核函数本质？提高维度，增加模型复杂度

# 滴滴

- 介绍xgboost、gbdt、rf的区别
- 树模型的特征选择中除了信息增益、信息增益比、基尼指数这三个外，还有哪些？
- Sklearn中树模型输出的特征重要程度是本身的还是百分比？
- 介绍下SVM以及它的核函数
- 熟悉FM算法不
- 算法题：两个链表的第一个公共节点

- 进程和线程的区别？
- HBase数据库的优点？

# xx

- 算法题：两个数字链表求和，将结果也存到一个链表里面，注意相加超10时进位就行
- RF与xgboost的区别？怎样选取的特征？如何判断这些特征的重要程度？最后RF的层数和深度是多少？
- 还用了深层神经网络？几层？用GPU没？特征维度到底多少？服务器配置？啥？你能把全部数据放进内存？

# 乐信

- 关联规则中，置信度和支持度的概念？
- MySQL中MYISAM和InnoDB的区别
- LR，svm，rf等算法的区别
- 模型评价指标，解释AUC，准确率和召回率
- 对于同一个数据，怎样根据AUC判断模型的好坏？数据？

# 新浪门户

- 介绍LR，为啥用的是似然函数不用最小二乘？当用lr时，特征中的某些值很大，意味着这个特征重要程度很高？对吗？不对，用lr时需要对特征进行离散化。。。
- L1和L2正则的区别？
- 树模型中，特征选择方法有哪些？ID3和C4.5分裂后，节点的信息熵是变大还是变小？变小
- RF和gbdt的区别
- 介绍下深度学习，CNN中的卷积和池化
- Hadoop中shuffle过程

# 58到家

- 了解贝叶斯不？它的应用场景都有哪些？
- 知道哪些深度学习的框架？
- 网络分几层？TCP和UDP区别？写个快排吧
- 场景题：一个10T的文本，一个10M的文本，从大文本中找出与小文本中相似度大于80%的文本，提示，用SameHash
- 场景题：北京市所有小区的客户发出家政请求的可能性(回归问题)；或者从家政的全部业务流程中找出一个具体场景进行分析：家政阿姨接到派单通知后，进行家政服务的路径选择，可阿姨一天顶多服务2-3个家庭，该如何派单？

# 百度

- 怎样用的KNN进行的预测
- XGBoost与RF的区别
- RF的随机性体现在哪里？它的代码中输出的特征重要程度是怎么进行计算的？
- 实习项目中的评价标准是什么？accuracy和precision、recall这些一样吗？AUC的解释
- 了解哪些损失函数？区别是啥？
- 线性模型为何用的最小二乘作为损失函数而不用似然函数或者交叉熵？
- 了解哪些深度学习模型？keras底层用TensorFlow和theano时，代码有何不同？TensorFlow原理、流程图，session是啥？
- 编程题：两个数组的最长公共子序列和最长递增子序列，用DP写出来后

# 陌陌

- LR与SVM的区别
- GBDT与XGBoost的区别？
- 了解FM吗？GBDT的数据在使用前有什么需要注意的吗？
- 做过广告点击率预估没？LR+GBDT和GBDT+FM怎么结合的知道不？

- LR与GBDT的结合了解不
- 智力算法题(说是它的校招笔试题)：
    f(x)=p,y=0; 1-p,y=1 将这个概率函数转换为T(x)=1/2, y=0或者y=1
    提示：f(x)执行四次可能出现的结果有0,0：p^2 0,1:p(-1p) 1,0:p(1-p) 1,1(1-p)^2
    里面有两次结果的概率是一样的

# 美团

- 数据特征怎样选择的？怎样表示的？模型的选择？当时的模型参数是多少？结果效果如何？
- RF与GBDT的区别？为啥你要用集成的方法而不用准确度更高的算法模型？
- 推导LR
- 编程题：删除链表中倒数第K个节点，写出来后，面试官加难度：如果是个带环的链表呢？也就是先找到环的入口，再注意边界条件就行

- 为啥LR的输入特征一般是离散的而不是连续的？
- 了解各种优化算法不？梯度下降和随机梯度下降的区别？牛顿法和拟牛顿法的区别？为啥提出拟牛顿？因为牛顿法涉及海塞矩阵，它的逆矩阵求解很麻烦
- KNN的使用场景
- 智力题：
    1<=a,b<=99，甲手里有a+b的结果，乙手里有a*b的值，两人目前都不知道a和b的值，两人对话如下：
    1甲：你肯定不知道a，b的值是多少
    2乙：我好像知道了
    3甲：我好像也知道了
    问：a，b的值是多少
    思路：对话1说明a+b的可能性有多种组合，其对应的乘积也有多种组合，形成两个集合S和T
    对话2说明乙根据手里的乘积结果，可以得出S和T这两个集合的交集是唯一的
    对话3说明甲猜到了乙的想法，故也能猜出交集唯一。。。

# 凤凰网

- TCP和UDP的区别？
- 编程题：数组A[N+1]中每个数据的都是1<=x<=N，其中只有一个数字是重复的，请找出来。使用一些技巧性的方法，比如根据值找对应索引位置，将其变为负数，依次这样，知道发现要变的那个已经是负数，找到了。。。
- 解释rf，xgboost，gbdt的区别
- 编程题：从数组A中找出所有和为S的两个数的索引，leetcode 原题

- 顺时针打印矩阵：剑指offer上的原题

# 推荐系统

为推荐系统选择正确的推荐算法是非常重要的决定。目前为止，已经有许多推荐算法可供选择，但为你需要解决的特定问题选择一种特定的算法仍然很困难。每一种推荐算法都有其优点和缺点，当然也有其限制条件，在作出决定之前，你必须要一一考量。在实践中，你可能会测试几种算法，以发现哪一种最适合你的用户，测试中你也会直观地发现它们是什么以及它们的工作原理。

推荐系统算法通常是某类推荐模型的实现，它负责获取数据，例如用户的喜好和可推荐项的描述，以及预测给定的用户组会对哪些选项感兴趣。

推荐算法通常被分为四大类（1-4）：

- 协同过滤推荐算法
- 基于内容的推荐算法
- 混合推荐算法
- 流行度推荐算法

除了这些种类以外，还有一些高级非传统的推荐算法（5）。

推荐算法综述是分文五个部分的系列文章，本文作为第一篇，将会简要介绍推荐系统算法的主要种类。其中包括算法的简要描述、典型的输入、不同的细分类型以及其优点和缺点。在第二和第三篇中，我们将会详细介绍这些算法的区别，让你能够深入理解他们的工作原理。系列文章中的一些内容参考了一篇来自 RecSys 2014 tutorial 的文章：由[Xavier Amatriain](http://xavier.amatriain.net/)编写的[The Recommender Problem Revisited](http://www.slideshare.net/xamat/recsys-2014-tutorial-the-recommender-problem-revisited)。

### 1. 协同过滤推荐算法

- 简介：通过在用户的一系列行为中寻找特定模式来产生用户特殊推荐
- 输入：仅仅依赖于惯用数据（例如评价、购买、下载等用户偏好行为）
- 类型：
  - 基于邻域的协同过滤（基于用户和基于项）
  - 基于模型的协同过滤（矩阵因子分解、受限玻尔兹曼机、贝叶斯网络等等）
- 优点：
  - 需要最小域
  - 不需要用户和项
  - 大部分场景中能够产生足够好的结果
- 缺点：
  - 冷启动问题
  - 需要标准化产品
  - 需要很高的用户和项的比例（1：10）
  - 流行度偏见（有长尾的时候表现不够好）
  - 难于提供解释

### 2.基于内容的推荐算法

- 简介：向用户推荐和其过去喜欢项的内容（例如元数据、描述、话题等等）相似的项
- 输入：仅仅依赖于项和用户的内容/描述（除了惯用数据）
- 类型：
  - 信息检索（例如 tf-idf 和 Okapi BM25）
  - 机器学习（例如朴素贝叶斯、支持向量机、决策树等等）
- 优点：
  - 没有冷启动问题
  - 不需要惯用数据
  - 没有流行度偏见，可以推荐有罕见特性的项
  - 可以使用用户内容特性来提供解释
- 缺点：
  - 项内容必须是机器可读的和有意义的
  - 容易归档用户
  - 很难有意外，缺少多样性
  - 很难联合多个项的特性

### 3.混合推荐算法

- 简介：综合利用协同过滤推荐算法和基于内容的推荐算法各自的优点同时抵消各自的缺点
- 输入：同时使用用户和项的内容特性与惯用数据，同时从两种输入类型中获益
- 类型：
  - 加权
  - 交换
  - 混合
  - 特性组合
  - 案列
  - 特征增强
  - 元层次
- 优点：
  - 由于单独使用协同过滤推荐算法和基于内容的推荐算法
  - 没有冷启动问题
  - 没有流行度偏见，可推荐有罕见特性的项
  - 可产生意外，实现多样性
- 缺点：
  - 需要通过大量的工作才能得到正确的平衡

### 4.流行度推荐算法

- 简介：这是一种推荐流行项的方法(例如最多下载、最多看过、最大影响的项)
- 输入：使用惯用数据和项的内容（例如类目）
- 优点：
  - 相对容易实现
  - 良好的基准算法
  - 有助于解决新用户冷启动问题
- 缺点：
  - 需要标准化产品
  - 经常需要一些项的类型进行分类
  - 不会推荐新项（很少有机会被观测到）
  - 推荐列表不会改变太大

### 5.高级非传统推荐算法

- 类型：
  - 深度学习
  - 学习等级
  - Multi-armed bandits（探索/开发）
  - 上下文感知推荐
  - 张量分解
  - 分解机
  - 社会推荐
- 优点：
  - 利于勉强维持最终性能百分点
  - 你可以说你正在使用渐进的方式
- 缺点：
  - 难于理解
  - 缺乏推荐工具支持
  - 没有为你的首个推荐系统提供推荐的方式

## Reference

- [机器学习——推荐系统 知识点总结](https://blog.csdn.net/qq_34896915/article/details/75040767)
- [推荐系统杂谈](http://www.importnew.com/22728.html)
- [推荐算法和机器学习系列 - 推荐算法综述](https://blog.csdn.net/initphp/article/details/52368812)
- [面试：机器学习--推荐算法](https://blog.csdn.net/u013713010/article/details/44905669)
- [推荐系统问题总结](https://blog.csdn.net/xbmatrix/article/details/61476131)

# 数据挖掘、机器学习、深度学习、推荐算法的联系与差别总结

- 数据挖掘：data mining，是一个很宽泛的概念。字面的意思是从成吨的数据里面挖掘有用的信息。这个工作BI（商业智能）可以做，数据分析可以做，甚至市场运营也可以做。利用Excel分析数据，发现了一些有用的信息，然后通过这些信息指导你的Business的过程也是数据挖掘的过程。

- 机器学习：machine learning，是计算机科学和统计学的交叉学科，基本目标是学习一个x->y的函数（映射），来做分类或者回归的工作。之所以经常和数据挖掘合在一起讲是因为现在好多数据挖掘的工作是通过机器学习提供的算法工具实现的，例如广告的ctr预估，PB级别的点击日志在通过典型的机器学习流程可以得到一个预估模型，从而提高互联网广告的点击率和回报率；个性化推荐，还是通过机器学习的一些算法分析平台上的各种购买，浏览和收藏日志，得到一个推荐模型，来预测你喜欢的商品。

- 深度学习：deep learning，机器学习里面现在比较火的一个topic（大坑），本身是神经网络算法的衍生，在图像，语音等富媒体的分类和识别上取得了非常好的效果，所以各大研究机构和公司都投入了大量的人力做相关的研究和开发。

> 总结下，数据挖掘是个很宽泛的概念，数据挖掘常用方法大多来自于机器学习这门学科，深度学习是机器学习一类比较火的算法，本质上还是原来的神经网络。

- 推荐算法：机器学习是方法，人工智能/数据挖掘是应用，可以使用机器学习，也可以使用别的方法。数据挖掘有很多应用场景，推荐系统是其中一个业务目的明确的, 有一定历史的，成体系的，已经形成较为完善的经验积累的应用场景。数据挖掘中还有很多应用场景有待开发，连是否能挖出有价值的模式都不知道。像推荐系统，计算机视觉，nlp这些价值已知的场景，明显是要幸运一些。写书的当然什么都要写写啦，难道机器学习里有的东西，推荐系统的书就不能写了？再说这些书侧重点不同，推荐系统里讲的机器学习算法更贴近推荐业务，侧重于算法应用，应用效果，对具体业务指标的影响，给整个系统带来的局限或提升等等。肯定不会像统计学习基础那样给你讲一堆推倒过程，讲统计特性，更不会像统计学习理论那样再给你来几下证明。这些书都有说机器学习，但是角度不同。如果讲机器学习算法的书和讲推荐系统的书在讲机器学习算法时基本差不多，那么那本讲机器学习的书也基本可以扔掉了。 

# Interview Link

- [七月在线深度学习面试题***](https://www.julyedu.com/question/big/kp_id/26/ques_id/932) - 七月在线
- [那些深度学习《面试》你可能需要知道的](https://zhuanlan.zhihu.com/p/29936999) - 知乎
- [深度学习面试](https://blog.csdn.net/mieleizhi0522/article/details/80001770) - CSDN
- [爱奇艺-推荐算法面试](https://blog.csdn.net/u012559634/article/details/71178567) - CSDN
- [字节跳动（今日头条）推荐算法实习生面试](https://blog.csdn.net/program_developer/article/details/80340829) - CSDN
- [机器学习面试问题汇总](https://blog.csdn.net/q383700092/article/details/58605715)
- [Gainlo 面试指南](https://wizardforcel.gitbooks.io/gainlo-interview-guide/content/)
- [常见机器学习面试题](https://blog.csdn.net/xbmatrix/article/details/62056589)
- [怎么准备自然语言处理和推荐系统这一块的面试?](https://www.zhihu.com/question/26926435)





# 波士顿房价预测学习项目笔记

> - [波士顿房价预测学习项目笔记](https://www.cnblogs.com/1xin/p/6689880.html)