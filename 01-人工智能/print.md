## Index

<!-- TOC -->



<!-- /TOC -->

## 1. 机器学习项目流程

1. **数学抽象**

   指的是根据数据明确任务目标，是分类、还是回归，或者是聚类。

2. **数据获取**

   对于分类问题，数据偏斜不能过于严重（平衡）。数据要有代表性，否则必然会过拟合。对数据的量级要有一个评估，多少个样本，多少个特征，据此估算出内存需求。

3. **预处理与特征选择**

   这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。

4. **模型训练与调优**

   现在很多算法都能够封装成黑盒使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。

5. **模型诊断**

   过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。

6. **模型融合/集成**

   一般来说，模型融合后都能使得效果有一定提升。而且效果很好。

7. **上线运行**

   这一部分内容主要跟工程实现的相关性更大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。

## 2. 分类器的选择

如果训练集很小，那么高偏差/低方差分类器（如朴素贝叶斯分类器）要优于低偏差/高方差分类器（如 k 近邻分类器），因为后者容易过拟合。然而，随着训练集的增大，低偏差/高方差分类器将开始胜出（它们具有较低的渐近误差），因为高偏差分类器不足以提供准确的模型。

你也可以认为这是生成模型与判别模型的区别。

**某些分类器的优势**

**朴素贝叶斯(Naive Bayes, NB)**
超级简单，就像做一些数数的工作。如果条件独立假设成立的话，NB将比鉴别模型（如Logistic回归）收敛的更快，所以你只需要少量的训练数据。即使条件独立假设不成立，NB在实际中仍然表现出惊人的好。如果你想做类似半监督学习，或者是既要模型简单又要性能好，NB值得尝试。

**Logistic回归(Logistic Regression, LR)**
LR有很多方法来对模型正则化。比起NB的条件独立性假设，LR不需要考虑样本是否是相关的。与决策树与支持向量机（SVM）不同，NB有很好的概率解释，且很容易利用新的训练数据来更新模型（使用在线梯度下降法）。如果你想要一些概率信息（如，为了更容易的调整分类阈值，得到分类的不确定性，得到置信区间），或者希望将来有更多数据时能方便的更新改进模型，LR是值得使用的。

**决策树（Decision Tree, DT）**
DT容易理解与解释（对某些人而言——不确定我是否也在他们其中）。DT是非参数的，所以你不需要担心野点（或离群点）和数据是否线性可分的问题（例如，DT可以轻松的处理这种情况：属于A类的样本的特征x取值往往非常小或者非常大，而属于B类的样本的特征x取值在中间范围）。DT的主要缺点是容易过拟合，这也正是随机森林（Random Forest, RF）（或者Boosted树）等集成学习算法被提出来的原因。此外，RF在很多分类问题中经常表现得最好（我个人相信一般比SVM稍好），且速度快可扩展，也不像SVM那样需要调整大量的参数，所以最近RF是一个非常流行的算法。

**支持向量机（Support Vector Machine, SVM）**
很高的分类正确率，对过拟合有很好的理论保证，选取合适的核函数，面对特征线性不可分的问题也可以表现得很好。SVM在维数通常很高的文本分类中非常的流行。由于较大的内存需求和繁琐的调参，我认为RF已经开始威胁其地位了。

回到LR与DT的问题（我更倾向是LR与RF的问题），做个简单的总结：两种方法都很快且可扩展。在正确率方面，RF比LR更优。但是LR可以在线更新且提供有用的概率信息。鉴于你在Square(不确定推断科学家是什么，应该不是有趣的化身)，可能从事欺诈检测：如果你想快速的调整阈值来改变假阳性率与假阴性率，分类结果中包含概率信息将很有帮助。无论你选择什么算法，如果你的各类样本数量是不均衡的（在欺诈检测中经常发生），你需要重新采样各类数据或者调整你的误差度量方法来使各类更均衡。

**但是。。。**

更好的数据往往比更好的算法更重要，提取好的特征也需要很大的功夫。如果你的数据集非常大，那么分类算法的选择可能对最后的分类性能影响并不大（所以可以根据运行速度或者易用性来选择）。

如果你很在意分类的正确率，那么你得尝试多种分类器，根据交叉验证的结果来挑选性能最好的。或者，学习下Netflix Prize和Middle Earth, 使用某种集成的方法来组合多个分类器。

## 3. SVM 原理

























